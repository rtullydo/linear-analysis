<?xml version="1.0" encoding="UTF-8" ?>

<pretext xmlns:xi="http://www.w3.org/2001/XInclude">

    <docinfo>
        <macros>
        \DeclareMathOperator{\RE}{Re}
          \DeclareMathOperator{\IM}{Im}
          \DeclareMathOperator{\ess}{ess}
          \DeclareMathOperator{\intr}{int}
          \DeclareMathOperator{\dist}{dist}
          \DeclareMathOperator{\dom}{dom}
          \DeclareMathOperator{\diag}{diag}
          \DeclareMathOperator{\span}{span}
          \DeclareMathOperator{\null}{null}
          \DeclareMathOperator{\rank}{rank}
          \DeclareMathOperator{\col}{col}
          \DeclareMathOperator{\row}{row}
          \DeclareMathOperator{\proj}{proj}
          \DeclareMathOperator\re{\mathrm {Re~}}
          \DeclareMathOperator\im{\mathrm {Im~}}
          %\newcommand\half{\tfrac 12}
          \newcommand\dd{\mathrm d}
          \newcommand{\eps}{\varepsilon}
          \newcommand{\To}{\longrightarrow}
          \newcommand{\hilbert}{\mathcal{H}}
          \newcommand{\s}{\mathcal{S}_2}
          \newcommand{\A}{\mathcal{A}}
          \newcommand\h{\mathcal{H}}
          \newcommand{\J}{\mathcal{J}}
          \newcommand{\M}{\mathcal{M}}
          \newcommand{\F}{\mathbb{F}}
          \newcommand{\N}{\mathcal{N}}
          \newcommand{\T}{\mathbb{T}}
          \newcommand{\W}{\mathcal{W}}
          \newcommand{\X}{\mathcal{X}}
          \newcommand{\D}{\mathbb{D}}
          \newcommand{\C}{\mathbb{C}}
          \newcommand{\BOP}{\mathbf{B}}
          \newcommand{\Z}{\mathbb{Z}}
          \newcommand{\BH}{\mathbf{B}(\mathcal{H})}
          \newcommand{\KH}{\mathcal{K}(\mathcal{H})}
          \newcommand{\pick}{\mathcal{P}_2}
          \newcommand{\schur}{\mathcal{S}_2}
          \newcommand{\R}{\mathbb{R}}
          \newcommand{\Complex}{\mathbb{C}}
          \newcommand{\Field}{\mathbb{F}}
          \newcommand{\RPlus}{\Real^{+}}
          \newcommand{\Polar}{\mathcal{P}_{\s}}
          \newcommand{\Poly}{\mathcal{P}(E)}
          \newcommand{\EssD}{\mathcal{D}}
          \newcommand{\Lop}{\mathcal{L}}
          \newcommand{\cc}[1]{\overline{#1}}
          \newcommand{\abs}[1]{\left\vert#1\right\vert}
          \newcommand{\set}[1]{\left\{#1\right\}}
          \newcommand{\seq}[1]{\left\lt#1\right>}
          \newcommand{\norm}[1]{\left\Vert#1\right\Vert}
          \newcommand{\essnorm}[1]{\norm{#1}_{\ess}}
          \newcommand{\tr}{\operatorname{tr}}
          \newcommand{\ran}[1]{\operatorname{ran}#1}
          \newcommand{\nt}{\stackrel{\mathrm {nt}}{\to}}
          \newcommand{\pnt}{\xrightarrow{pnt}}
          \newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}
          \newcommand{\ad}{^\ast}
          \newcommand{\inv}{^{-1}}
          \newcommand{\adinv}{^{\ast -1}}
          \newcommand{\invad}{^{-1 \ast}}
          \newcommand\Pick{\mathcal P}
          \newcommand\Ha{\mathbb{H}}
          \newcommand{\HH}{\Ha\times\Ha}
          \newcommand\Htau{\mathbb{H}(\tau)}
          \newcommand{\vp}{\varphi}
          \newcommand{\ph}{\varphi}
          \newcommand\al{\alpha}
          \newcommand\ga{\gamma}
          \newcommand\de{\delta}
          \newcommand\ep{\varepsilon}
          \newcommand\la{\lambda}
          \newcommand\up{\upsilon}
          \newcommand\si{\sigma}
          \newcommand\beq{\begin{equation}}
          \newcommand\ds{\displaystyle}
          \newcommand\eeq{\end{equation}}
          \newcommand\df{\stackrel{\rm def}{=}}
          \newcommand\ii{\mathrm i}
          \newcommand{\vectwo}[2]
          {
             \begin{pmatrix} #1 \\ #2 \end{pmatrix}
          }
          \newcommand{\vecthree}[3]
          {
             \begin{pmatrix} #1 \\ #2 \\ #3 \end{pmatrix}
          }
          \newcommand\blue{\color{blue}}
          \newcommand\black{\color{black}}
          \newcommand\red{\color{red}}
          %\newcommand\red{\color{black}}
          \newcommand\nn{\nonumber}
          \newcommand\bbm{\begin{bmatrix}}
          \newcommand\ebm{\end{bmatrix}}
          \newcommand\bpm{\begin{pmatrix}}
          \newcommand\epm{\end{pmatrix}}
          \numberwithin{equation}{section}
          \newcommand\nin{\noindent}
          \newcommand{\nCr}[2]{\,_{#1}C_{#2}} % nCr
          \newcommand{\vec}[1]{{\bf #1}}
          \newcommand{\ps}{\displaystyle \sum_{n=0}^\infty a_n x^n}
          \newcommand{\psg}{\displaystyle \sum_{n=0}^\infty b_n x^n}
          \newcommand{\hz}{\,\mathrm{Hz}}
        </macros>
    </docinfo>

<book xml:id="linanal">
    <title>Math 344 - Linear Analysis II Notes</title>
    <frontmatter>

        <titlepage>
            <author>
              <personname>Ryan Tully-Doyle</personname>
              <institution>Cal Poly, SLO</institution>
            </author>
            <date><today /></date>
        </titlepage>
    </frontmatter>

    <chapter xml:id="ch-0"><title>What you should know</title>
      <section><title>Introduction</title>
        <p>This course is the second course in a sequence that uses the language and techniques of linear algebra to study problems in differential equations. The following sections give a broad overview of ideas from calculus, differential equations, and linear algebra that will be used in the course. An overview of ideas you should have seen before might look like</p>
          <ol>
            <li><p>Calculus
              <ul>
                <li>Taylor series</li>
                <li>Interval of convergence and the ratio test</li>
                <li>Power series of basic functions</li>
                <li>Integrating and differentiating power series</li>
                <li>Improper integrals</li>
              </ul></p></li>
              <li><p>Differential equations
                <ul>
                <li>Second order constant coefficient linear differential equations</li>
                <li>Characteristic equations</li>
                <li>Reduction of order</li>
                <li>Harmonic motion (homogeneous and forced motion)</li>
                </ul></p></li>
              <li><p>Linear Algebra
                <ul>
                  <li>Vector spaces</li>
                  <li>Linear independence</li>
                  <li>Basis of a vector space</li>
                  <li>Inner products (beyond the dot product)</li>
                  <li>Orthogonality</li>
                  <li>Vector projections</li>
                </ul></p></li>
              </ol>
        <p>You don't need to be an expert, but having seen and used these ideas in the past is going to make this course quite a bit more interesting and productive.</p>
        </section>

      <section><title>Calculus</title>
      <p>The first of the basic ideas we're going to need is <term>improper integration</term>, which is a fancy way of saying an integral with infinity as a limit. Do not be fooled by the notation
      <me>
        \int_a^\infty f(x) \, dx.
      </me>
      Infinity is not a number, and the notation for improper integrals is merely shorthand for a limit (this is a recurring theme with infinity). That is, we say that an improper integral in infinity <term>converges</term> if
      <me>
        \int_a^\infty f(x) \, dx = \lim_{N \to \infty} \int_a^N f(x) \, dx \text{ exists and is finite}.
      </me>
      <em>DO NOT</em> treat infinity like a number.</p>

      <p>An infinite series is also a limit. (The clue is that infinity is present in the symbol). That is,
        <me>
          \sum_{n=0}^\infty a_n = \lim_{N \to \infty} \sum_{n=a}^N a_n.
        </me>
      This is sometimes called the <q>limit of partial sums</q>. An infinite series exists if the limit of partial sums exists and is finite.    An infinite series <m>\sum a_n</m> is said to <term>converge absolutely</term> if <m>\sum \abs{a_n}</m> is a convergent series.</p>

      <p>You learned many ways of determining when an infinite series converges in calculus. The most important test for us in this course is the <term>ratio test</term>, which states that a series converges (absolutely) if
      <me>
      \lim_{n \to \infty} \abs{\frac{a_{n+1}}{a_n}} \lt \infty.
      </me>
   
      </p>
      </section>

         <section><title>Linear algebra</title>
          <subsection xml:id="sec-intro-1"><title>Motivation</title>
            <p>One of the most profound ideas of linear algebra is that <em>any finite dimensional vector space over <m>\R</m> or <m>\C</m> is secretly <m>\R^n</m> or <m>\C^n</m></em>. This insight allows us to reduce the study of vector spaces and the maps between them to the study of matrices.</p>

            <p>The key idea is that every finite dimensional vector space can be represented in coordinates once we choose a basis. We denote the representation of a vector <m>v \in V</m> with respect to a basis <m>\mathcal V</m> by <m>v_\mathcal{V}</m>. Better yet, that basis can be chosen to be orthonormal by way of the Gram-Schmidt process and the dot product structure of Euclidean space. The coordinatization of <m>V</m> also gives unique representations of linear maps betwen those spaces.</p>

            <theorem>
                <p>Let <m>V, W</m> be finite dimensional vector spaces with bases <m>\mathcal V, \mathcal W</m>. Then any linear map <m>T: V \to W</m> has a unique matrix representation with respect to <m>\mathcal V, \mathcal W</m> by
                    <me>
                        T(x) = A x
                    </me>
                with
                    <me>
                        A = \bbm T(v_1)_{\mathcal W} \amp \ldots \amp T(v_n)_{\mathcal W} \ebm
                    </me>
                </p>
            </theorem>

            <p>Typical examples introduced in a linear algebra course include the space of polynomials of degree less than or equal to <m>n</m>. At the same time, we usually also get to see a very suggestive example of a useful linear map and the representation of that map in matrix form.</p>

            <p>Let <m>P_n</m> denote the space of polynomials of degree <m>\leq n</m>. Consider the map <m>D: P_3 \to P_2</m> defined by 
                <me>
                    D(a_0 + a_1 t + a_2 t^2 + a_3 t^3) = a_1 + 2 a_2 t + 3 a_3 t^2.
                </me>
            That is, <m>D</m> is the map that takes the derivative of a polynomial. It isn't hard to use the standard basis for <m>P_n</m> to get the matrix representation
                <me>
                    D(p) = \bbm 0 \amp 1 \amp 0 \amp 0 \\ 0 \amp 0 \amp 2 \amp 0 \\ 0 \amp 0 \amp 0 \amp 3 \ebm \bbm a_0 \\ a_1 \\ a_2 \\ a_3 \ebm
                </me>
            for the action of <m>D</m> on <m>P_3</m>. 
            </p>

            <p>This example is a good place to begin asking questions about how far we can push finite dimensional linear algebra. The fact that differentiation of polynomials is wonderful - but what else can we apply it to? Nice functions have power series that converge absolutely, and we like to think of an absolutely convergent power series as sort of an <q>infinite polynomial</q>. Our intution might lead us to make a connection with calculus at this point. When we learn to work with power series, we learn that for a convergent power series,
                <me>
                    \frac{d}{dx} \sum a_n (x-a)^n = \sum n a_n (x-a)^{n-1}.
                </me>
            In analogy with our example about polynomials above, we're tempted to write, for a function <m>f</m> defined by a convergent power series, that
                <me>
                    D(f) =  \underbrace{\bbm 0 \amp 1 \amp 0 \amp 0 \amp \ldots \\ 0 \amp 0 \amp 2 \amp 0 \amp \ldots \\ 0 \amp 0 \amp 0 \amp 3 \amp \ldots \\ \vdots \amp \amp \amp \amp \ddots \ebm}_{A} \bbm a_0 \\ a_1 \\ a_2 \\ a_3 \\ \vdots \ebm = a_1 + 2a_2 (x - a) + 3 a_3 (x-1)^2 + \ldots.
                </me>
            </p>

            <p>This idea is shot through with issues that need to be addressed. 
                <ul>
                    <li>The object <m>A</m> is some kind of <m>\infty \times \infty</m> matrix. How does that make sense?</li>
                    <li> What are the vector spaces that <m>D</m> is mapping between?</li>
                    <li> Does the idea of coordinatization still work?</li>
                    <li>If it does, what exactly is <q><m>\R^\infty</m></q> supposed to be?</li> 
                    <li>Do infinite dimensional vector spaces and bases make sense at all?</li>
                </ul>
            </p>

        </subsection>

        <subsection><title>Inner products</title>
            <p>The <term>dot product</term> of two vectors in <m>\C^n</m> is
            <men xml:id="def-dot">
                x \cdot y = \sum_{i=1}^n \cc y_i x_i.
            </men>
            Standard notation for the dot product is <m>\ip{x}{y}</m> and in <m>\C^n</m> is equivalent to <m>y\ad x</m>, where <m>\ad</m> designates the conjugate transpose of a matrix. The dot product has the following properties:
            <ol>
                <li>
                    <m>\ip{x}{y} = \cc{\ip{y}{x}} \hspace{.2in} \text{conjugate symmetry}</m>
                </li>
                <li>
                    <m>\ip{x + y}{z} = \ip{x}{y} + \ip{y}{z} \hspace{.2in} \text{linearity in the first term}</m>
                </li>
                <li>
                    <m>\ip{x}{x} \geq 0 \hspace{.2in} \text{non-negativity}</m>
                </li>
            </ol>
            </p>

            <p>Once we have the dot product, we can start building the geometry of <m>\C^n</m>. First, note that 
            <men xml:id="eq-Euclidean-norm">
                \norm{x}^2 = \ip{x}{x} = \sum_{i=1}^n \abs{x}^2.
            </men>
            Motivated by the real case, we say that two vectors <m>x, y</m> are <term>orthogonal</term> and write <m>x \perp y</m> if <m>\ip{x}{y} = 0</m>. 
            </p>

            <p>Another important inequality is indicated by the relationship between angles and the dot product in <m>\R^n</m>, where we have
            <me>
                \ip{x}{y} = \norm{x}\norm{y}\cos \theta,
            </me>
            where <m>\theta</m> is the angle between the vectors. While the idea of <q>angle</q> doesn't make sense in <m>\C^n</m> (at least in the same way), we still have the <term>Cauchy-Schwarz</term> inequality
            <men>
                \abs{\ip{x}{y}} \leq \norm{x}\norm{y}.
            </men></p>

            <p>Orthogonality also underlies the vector version of the <term>Pythagorean theorem</term>, 
            <men>
                \norm{x}^2 + \norm{y}^2 = \norm{x+ y}^2 \iff x\perp y.
            </men></p>

            <p>Finally, it would be remiss to leave out the single most important inequality in mathematics, our old friend the <term>triangle inequality</term>, which in vector terms can be expressed
            <men>
                \norm{x + y} \leq \norm{x} + \norm{y}
            </men>
            </p>

            <p>Because finite dimensional vector spaces have representations in coordinates as <m>\R^n</m> or <m>\C^n</m>, all finite dimensional vector spaces carry the geometric structure delineated above.</p>
        </subsection>

      <subsection xml:id="intro-2"><title>Basis and coordinates</title>
          <p>Let <m>V</m> be a vector space over a field <m>\F</m>. Recall that a (finite) set of vectors <m>S \subset V</m> is <term>linearly independent</term> if only the trivial solution exists for the equation
          <men>
              0 = \sum_\mathcal{I} c_i v_i.
          </men>
          A set <m>S</m> of vectors in <m>V</m> is said to <term>span</term> <m>V</m> if every vector in <m>V</m> can be realized as a linear combination of vectors in <m>S</m>. That is, given <m>v \in V</m>, there exist coefficients <m>c_i</m> so that 
          <me>
              v = \sum_{\mathcal I} c_i v_i.
          </me>
          </p>


          <p>A basis <m>\mathcal V</m> for <m>V</m> is a subset of <m>V</m>
          so that <m>\mathcal V</m> is linearly independent and <m>\mathcal V</m> spans <m>V</m>. It is a major result that every vector space has a basis. The full result requires the invocation of <url href="https://en.wikipedia.org/wiki/Zorn%27s_lemma">Zorn's Lemma</url> or other equivalents of the <url href="https://en.wikipedia.org/wiki/Axiom_of_choice">axiom of choice</url> and will not be proven here. (A nice argument can be found <url href="http://www.math.lsa.umich.edu/~kesmith/infinite.pdf">here</url>.) Our interest is in modeling vector spaces the carry the logic and structure of Euclidean space. The <term>dimension</term> of <m>V</m> is the order of a basis <m>\mathcal V</m>. If the basis has a finite number of elements, say <m>n</m>, then <m>V</m> is called finite dimensional. In particular, (and clearly providing motivation for the definition), <m>\dim \R^n = n</m>.</p>

          <p>Suppose that <m>V</m> is a finite dimensional vector space with a basis <m>\mathcal V</m>. Let <m>v</m> be a vector in <m>V</m>. Then the <term>coordinates of <m>v</m> with respect to <m>\mathcal V</m></term> are the constants <m>c_i</m> so that <m>v = \sum_{\mathcal I} c_i v_i</m>. These coordinates are <em>unique</em> once we have fixed a basis <m>\mathcal V</m>. That is, we have a bijective relationship between the vectors <m>v \in V</m> and the coordinate representations <m>\bbm c_1 \\ \vdots \\ c_n \ebm \in \F^n</m>. In <m>\F^n</m>, the coordinate representation of a vector is straightforward to compute using the dot product.</p>

          <theorem xml:id="thm-finite-coords">
              <p>Let <m>e_1, \ldots e_m</m> be an orthonormal basis for <m>\F^m</m> and <m>v \in \F^n</m>. Then the <m>n</m>th coordinate of <m>v</m> with respect to the basis is <m>\ip{v}{e_n}</m>, and the expansion of <m>v</m> with respect to the basis is
              <me>
                  v = \sum_{1}^m \ip{v}{e_i} e_i.
              </me></p>
          </theorem>

          <p>Furthermore, we can use the coordinate representation to write representing matrices for linear functions <m>T:V \to W</m>. Suppose that <m>V, W</m> are vector spaces of dimension <m>m,n</m> respectively over <m>\F</m>. Then
              <sidebyside width="30%">
              <image>
              <latex-image >
              \begin{tikzcd}
                  V \arrow[r,"T"] \arrow{d}{i_V} \amp W \\
                  \F^m \arrow[r,"A"] \amp \F^n \arrow{u}{i_W \inv}
              \end{tikzcd}
              </latex-image>
              </image>
              </sidebyside>
          where <m>A</m> is the matrix that represents <m>T</m> and <m>i</m> is the natural bijection- the coordinatization - between <m>V, W</m> and <m>\F^m, \F^n</m> respectively. We should note that matrix multiplication is defined so that
              <sidebyside width="40%">
              <image>
              <latex-image >
              \begin{tikzcd}
                  U \arrow{r}{S} \arrow{d}{i_U} \amp V \arrow[r,"T"]  \amp W \\
                  \F^r \arrow{r}{A} \amp \F^m \arrow[r,"B"] \amp \F^n \arrow{u}{i_W \inv}
              \end{tikzcd}
              </latex-image>
              </image>
              </sidebyside>
              reduces to the diagram
              <sidebyside width="30%">
              <image>
              <latex-image >
              \begin{tikzcd}
                  U \arrow[r,"S \circ T"] \arrow{d}{i_U} \amp W \\
                  \F^r \arrow[r,"BA"] \amp \F^n \arrow{u}{i_W \inv}
              \end{tikzcd}
              </latex-image>
              </image>
              </sidebyside>
          That is, the representing matrix of a composition is the product of the representing matrices of the functions.
          </p>

          <p>Any basis of a vector space can be replaced with an equivalent basis of orthonormal vectors - the algorithm for creating an orthonormal basis from a basis is called the <term><url href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram-Schmidt process</url></term>.</p>
      </subsection>

      <subsection><title>Operators</title>
          <p>When a linear function maps <m>V</m> into itself, special things happen. First, the matrix that represents <m>T: \F^n \to \F^n</m> is square. There are a large number of equivalences between the structure of square matrices, linear maps, and sets of vectors. Many of these are captured in the <term>invertible matrix theorem</term>, one of the central objects of study in elementary linear algebra.
          </p>

          <theorem xml:id="thminvmat1">
          <title>Invertible matrix theorem</title>
          <p>Let <m>A</m> be an <m>n \times n</m> matrix. If any of the following conditions hold, all of them do. If any of them are false, they are all.
          <ol>
            <li><m>A</m> is invertible.</li>
            <li><m>A</m> row reduces to the identity matrix <m>I</m>.</li>
            <li><m>A</m> has <m>n</m> pivot positions.</li>
            <li><m>rank A = n</m>.</li>
            <li>The equation <m>A \vec x = \vec 0</m> has only the trivial solution.</li>
            <li>The columns of <m>A</m> are linearly independent.</li>
            <li>The function <m>T(\vec x) = A \vec x</m> is one-to-one.</li>
            <li>The equation <m>A \vec x = \vec v</m> is consistent for all <m>b \in \F^n</m>.</li>
            <li>The columns of <m>A</m> span <m>\F^n</m>.</li>
            <li>The function <m>T(\vec x) = A \vec x</m> is onto.</li>
            <li>There is a matrix <m>C</m> so that <m>C A = I</m>. </li>
            <li>There is a matrix <m>D</m> so that <m>A D = I</m>.</li>
            <li><m>A^T</m> is invertible.</li>
            <li><m>\det A \neq 0.</m></li>
          </ol></p>
      </theorem>

      <p>Operators contain more information than the invertibility of the functions that they represent. For the following discussion, let us fix a basis of a vector space <m>V</m> and let <m>A</m> be the matrix that represents a function <m>T: V \to V</m>. A scalar <m>\la</m> and a vector <m>v</m> are said to be an <term>eigenpair</term> for <m>A</m> if
      <me>
          A v = \la v.
      </me>
      It is straightforward to see that the set of all vectors <m>v</m> for which the eigenvector equation holds is a subspace of <m>V</m>, called the <term>eigenspace</term> associated with <m>\la</m>. The eigenspaces of the matrix <m>A</m> are its <term>invariant subspaces</term>, which is to say that a vector in an eigenspace is mapped by <m>A</m> to the same eigenspace. It turns out that knowing the invariant subspaces of <m>A</m> are often enough to completely characterize <m>A</m>. If <m>A</m> is <m>n\times n</m> and <m>A</m> has <m>n</m> linearly independent eigenvectors (that is, one can find a basis of <m>\F^n</m> consisting of eigenvectors of <m>A</m>), then 
      <me>
          A = S D S\inv,
      </me>
      where <m>S</m> is a matrix of eigenvectors and <m>D</m> is a diagonal matrix of the associated eigenvalues (including repetition of course). (One should think of <m>S</m> as a change of basis matrix under which the operator <m>A</m> becomes diagonal.) </p>

      <p>Many operators are not diagonalizable, even very simple ones. For example, <m>A = \bbm 1 \amp 1 \\ 0 \amp 1 \ebm</m> only has a one-dimensional eigenspace. Diagonalizability is so useful that we give characterizations of those operators a special name, the <term>Spectral Theorem</term>. An operator on a real vector space is called <term>symmetric</term> if <m>A^T = A</m>. An operator on a complex vector space is called <term>Hermitian</term> (or conjugate symmetric) if <m>A\ad = \cc{A^T} = A</m>. One of the major theorems of elementary linear algebra is that such operators are diagonalizable and that there exists an orthonormal basis of eigenvectors for <m>V</m>. </p>

      <theorem>
          <p>Let <m>A</m> be an <m>n \times n</m> real (complex) matrix. Then <m>A</m> is diagonalizable with respect to an orthonormal basis of eigenvectors if and only if <m>A</m> is symmetric (hermitian).</p>
      </theorem>

      <p>For complex operators, one can say more. <m>A</m> is called <term>normal</term> if <m>A A\ad = A\ad A</m>. One reason that complex vector spaces are so much nicer than real vector spaces is that normal operators turn out to have orthonormal diagonalizations.</p>

      <theorem><title>Complex (finite) spectral theorem</title>
          <p>Let <m>A</m> be an operator on a finite dimensional Hilbert space <m>V</m>. Then <m>A</m> is normal if and only if <m>A</m> can be diagonalized with respect to an orthonormal basis of eigenvectors for <m>V</m>.</p>
      </theorem></subsection>

    </section>
    </chapter>

    <chapter xml:id="ch-series">
        <title>Series solutions</title>
        <section xml:id="ch-series-1">
          <title>Power series</title>
          <subsection><title>Motivation</title>
          <p>One of the limitations of the elementary techniques of differential equations is the extremely limited number of types of equations that can be solved. Once we are dealing with equations that are second order or higher, we are pretty much restricted to equations with constant coefficients (<m>ay'' + by' + cy = 0</m>), or equations in special forms (like the Cauchy-Euler equations <m>ax^2y'' + bxy' + cy = 0</m>).</p>

          <example><title>Airy's equation</title>
            <p>A seemingly simple equation that cannot be solved with elementary techniques is <term>Airy's equation</term>, which is of the form
            <me>
              y'' - xy = 0.
            </me>
            </p>
          </example>

          <p>One problem that we run into is that many functions cannot be expressed as combinations of the elementary functions that we learn in calculus (for example, <m>f(x) = \int e^{x^2}\, dx</m>), and these functions are frequently the solutions to differential equations like the one in the example above. We turn, as we did in calculus, to <term>power series</term> representations of functions. Power series are one of the most important ideas developed in introductory calculus (though it is often the case that WHY you are learning about them isn't immediately obvious). We'll begin by recalled the definition of a power series, the question of when and where such a series converges, and some basic operations that we can perform</p>
        </subsection>
        <subsection><title>Definition of power series and convergence</title>
          <definition xml:id="def-powerseries">
            <p>An infinite series of the form
              <me>
                \sum_{n=0}^\infty a_n (x - x_0)^n
              </me>
              is called a power series centered at <m>x = x_0</m>.
            </p>
             <p>Because we can always make the substitution <m>u = x-x_0</m>, it is typically sufficient to consider series centered at <m>0</m>,
            <men xml:id="def-powerseries-zero">
              \sum_{n=0}^\infty a_n x^n.
            </men>
          </p>
          </definition>

          <p>A power series <term>converges</term> at a point <m>x</m> if the sequence of partial sums converges - that is,
          <me>
            \lim_{N \to \infty} \sum_{n = 0}^N a_n x^n
          </me>
          exists and is equal to a finite value. The <term>interval of convergence</term> of a power series (centered at 0) is the largest interval of the form <m>I = (-r,r)</m> for which the series converges for each <m>x \in (-r,r)</m>. The quantity <m>r</m> is called the <term>radius of convergence</term>. There are three possibilities for the value of <m>r</m>.</p>

          <theorem xml:id="thm-power-converge">
            <p>For a series of the form given in <xref ref="def-powerseries-zero"/>, exactly one of the following holds.
            <ol>
              <li><m>r = 0:</m> <m>\ps</m> converges only for <m>x = 0</m>.</li>
              <li><m>r = \infty:</m> <m>\ps</m> converges for every <m>x \in \R</m>.</li>
              <li><m>r</m> is finite: There is a constant <m>r > 0</m> so that <m>\ps</m> converges for <m>x \lt \abs{r}</m> and diverges for <m>x > \abs{r}</m>.</li>
            </ol></p>
          </theorem>

          <p>The tool from calculus that is most useful in computing a radius of convergence for a power series is the <term>ratio test</term> (the reason is that only a single <m>x</m> will remain after computing the ratio).</p>

          <definition>
            <p>For the power series <m>\ps</m>, the radius of convergence <m>r</m> is given by <m>r = \frac{1}{L}</m> where <m>L</m> is the limit
            <me>
              \lim_{n\to\infty} \abs{\frac{a_{n+1}}{a_n}} = L.
            </me>
            If <m>L = 0</m>, then <m>r = \infty</m> (that is, the series converges for all values of <m>x</m>). If <m>L = \infty</m>, then <m>r = 0</m> (that is, the series only converges trivially at <m>x = 0</m>).</p>
          </definition>

          <example><title>Finding the radius of convergence</title>
            <p>Compute the radius of convergence of
              <me>
                \sum_{n=0}^\infty \frac{2^{n+1}}{5^n} x^n.
              </me>
            </p>
            <p>
              We compute the limit
              <me>
                \lim_{n\to \infty}\abs{\frac{a_{n+1}}{a_n}} = \lim_{n\to \infty} \frac{2^{n+2}}{5^{n+1}} \cdot \frac{5^n}{2^{n + 1}} = \frac{2}{5},
              </me>
              and so <m>r = \frac{5}{2}</m>.</p>
            </example>
          </subsection>
          <subsection><title>Algebra and calculus of power series</title>
            <p>The beauty and utility in power series is that when they converge, they can be treated almost like giant polynomials. That is, we can perform algebra and calculs operations on convergent power series. To do so, first we can make a function out of a power series by restricting the domain to the interval of convergence (so that the function is well-defined). That is, if <m>r</m> is the radius of convergence for <m>\ps</m>, then we can define
            <me>
              f(x) = \ps, \hspace{1cm} x \in (-r,r).
            </me></p>

            <p>Before we discuss operations involving power series, we note a fundamental fact (related to the identical fact about polynomials) - two power series are equal if and only if their corresponding coefficients are equal. That is,
              <me>
                \ps = \sum_{n=0}^\infty b_n x^n \text{ if and only if } a_n = b_n \text{ for all } n.
              </me>
            </p>

            <p>When we combine power series, we have to make sure that we work on a domain where both functions are defined - that is, if <m>f</m> and <m>g</m> have radii of convergence <m>r_1, r_2</m> respectively, then the radius of convergence of an algebraic combination of <m>f,g</m> will be <m>r = \min\{r_1,r_2\}</m>.</p>

            <assemblage>
              <p>Let <m>f(x) = \ps</m> and <m>g(x) =\psg</m> with common radius of convergence <m>r</m>.
              <ol>
                <li>Addition: <m>f(x) +g(x)  = \displaystyle\sum_{n=0}^\infty (a_n + b_n) x^n</m>.</li>
                <li>Scalar multiplication: <m>c f(x) = \displaystyle\sum_{n=0}^\infty (ca_n) x^n</m></li>
                <li>Multiplication of series: <m>f(x) g(x) = \displaystyle\sum_{n=0}^\infty c_n x^n</m> where the coefficients <m>c_n</m> are computed by <m>c_n = \displaystyle \sum_{k=0}^n a_{n-k} b_k</m> (this is essentially the result of a giant distribution and collection of like terms)</li>
              </ol>
              Division can be defined as well (as long as the function in the denominator is not zero), but is used less often in what we will be studying
              </p>
            </assemblage>

            <p>One of our first applications of power series will be to plug them into equations, in order to figure out the coefficients. If we know the coefficients of a function given by a power series, then we know the function. Typically, finding the coefficients will involve solving a <term>recurrence relation</term>, which is a recursive process for determining coefficients from some number of known coeffients.</p>

            <example><title>Solving a recurrence relation</title>
            <p>Assume that the coefficients in the power series 
              <me>
                f(x) = \ps
              </me>
              satisfy the equation
              <me>
                \sum_{n=1}^\infty n a_n x^n - \sum_{n=0}^\infty a_n x^{n+1} = 0.
              </me>
              Find a formula that gives any <m>a_n</m> in terms of <m>a_0</m>.
              </p>

              <p>First, to combine power series, we'll need both the powers of the like terms to match, and the series to have the same number of terms. Notice that if we replace <m>n \rightarrow n + 1</m> in the second series, and we change the range of the index accordingly, we get
              <me>
                \sum_{n=1}^\infty n a_n x^n - \sum_{n=1}^\infty a_{n-1} x^{n} = 0.
              </me>
              As the powers of the corresponding terms match, and the range of indicies match, we can combine the series using addition to get
              <me>
                \sum_{n=1}^\infty (na_n - a_{n-1}) x^n = 0.
              </me>
              Because a convergent power series can only be equal to zero if the coefficients are all zero, we get the recurrence relation
              <me>
                na_n - a_{n-1}  = 0, \hspace{1cm} n = 1, 2, 3, \ldots,
              </me>
              which can be written
              <me>
                a_n = \frac{1}{n} a_{n-1}, \hspace{1cm} n = 1, 2, 3 \ldots.
              </me>
              This gives a family of equations that we can use to find each coefficient in terms of <m>a_0</m> and eventually guess a formula that won't require us to work recursively.</p>

              <md>
                <mrow>\amp n = 1: \,\,\, a_1 = a_0.</mrow>
                <mrow>\amp n = 2: \,\,\, a_2 = \frac{1}{2}a_1 = \frac{1}{2} a_0.</mrow>
                <mrow>\amp n = 3: \,\,\, a_3 = \frac{1}{3}a_2 = \frac{1}{3\cdot 2} a_0.</mrow>
                <mrow>\amp n = 4: \,\,\, a_4 = \frac{1}{4}a_3 = \frac{1}{4\cdot 3 \cdot 2} a_0.</mrow>
              </md>

              <p>We can guess that the general formula for <m>a_n</m> is 
              <me>
                a_n = \frac{1}{n!} a_0.
              </me>
              (Notice that I haven't claimed to have proved this, because that would require mathematical induction. For our purposes in this course, if a pattern looks to hold, we will assume that it continues indefinitely.)</p>

              <p>Under the assumption that our formula for <m>a_n</m> holds for all <m>n</m>, we can say 
              <me>
                f(x) = a_0 \sum_{n=0}^\infty \frac{1}{n!} x^n,
              </me>
              which you might notice is the Taylor series at 0 for the function
              <m>f(x) = a_0 e^x</m>.</p>
            </example>

            <p>Just like algebra, we can perform calculus on power series in the obvious way (treating them like giant polynomials). Suppose that <m>f</m> is defined by a power series with domain equal to the interval of convergence, so that
            <me>
              f(x) = \ps, \,\,\, \abs{x} \lt r.
            </me> 
            Then we can differentiate <m>f</m> term by term to any order of derivative (after all, we're never going to run out of <m>x</m>es):

            <md>
              <mrow>f'(x) \amp= \sum_{n=1}^\infty n a_n x^{n-1}</mrow>
              <mrow>f''(x) \amp = \sum_{n=2}^\infty n(n-1) a_n x^{n-2}</mrow>
            </md>
            and so on. Power series can also be integrated, which is straightforward to write a formula for. (You might consider doing it as an exercise.)</p>
          </subsection>

          <subsection><title>Analytic functions (a connection with complex analysis)</title>
            <p>A function with a power series defined on some interval <m>(a,b)</m> is particularly nice because the power series implies the existence of derivatives of all orders. Such functions are called <term>smooth</term>. (Note that smooth functions need not have a convergent power series, which is a weird but deep fact of real analysis!). Power series representations are so special, we name functions that have them as a special class.</p>

            <definition xml:id="def-analytic">
              <p>A function <m>f</m> with a convergent power series representation on some non-trivial interval of convergence <m>(a,b)</m> is called an <term>analytic</term> function on <m>(a,b)</m>.</p>
            </definition>

            <p>Analytic functions are the nicest behaved functions in calculus and its applications. One of the major theorems of calculus is that if a function is analytic, then it has a unique power series representation, called the <term>Taylor series</term> for <m>f</m>. Even better, the series can be computed!</p>

            <me>
              f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!} (x  -x_0)^n.
            </me>

            <p>The functions of introductory calculus are nearly all analytic, many on the entire real line. You should be familiar with the power series expansions of those functions derived from exponential functions.</p>

            <assemblage>
              <p><ol>
                <li> <m>\displaystyle e^x = 1 + x + \frac{1}{2!} x^2 + \frac{1}{3!} x^3 + \ldots = \sum_{n=0}^\infty \frac{1}{n!} x^n</m></li>
                <li> <m>\displaystyle \cos x = 1 - \frac{1}{2!} x^2 + \frac{1}{4!} x^4 + \ldots =   \sum_{n=0}^\infty \frac{(-1)^n}{(2n)!} x^{2n}</m></li>
                <li> <m>\displaystyle \sin x = x - \frac{1}{3!} x^3 + \frac{1}{5!} x^5 + \ldots =   \sum_{n=0}^\infty \frac{(-1)^n}{(2n + 1)!} x^{2n+ 1}</m></li>
              </ol></p>
            </assemblage>

            <p>Because we can combine power series using algebra to get new power series, we can do the same with the analytic functions that they represent. That is, if <m>f, g</m> are analytic, so are algebraic combinations of them.</p>

            <theorem>
              <p>If <m>f, g</m> are analytic functions on a common interval <m>(a,b)</m>, then so are <m>f + g, f - g, fg</m>, and <m>\frac{f}{g}</m> (as long as <m>g(x) \neq 0</m>).</p>
            </theorem>

            <p>The most useful examples of analytic functions are polynomials! (From this persepective, polynomials are just finite power series.) Polynomials are analytic at every point, and so therefore are <m>f + g, f - g</m> and <m>f g</m>. Things are a bit more complicated in the case of rational functions <m>\frac{f}{g}</m>, because the zeroes of <m>g</m> determine the radius of convergence of the quotient.</p>

            <p>Here we come across one of the first places that the analysis of analytic functions is actually secretly taking place in the complex plane. Consider the function
              <me>
                f(x) = \frac{1}{1+x^2}.
              </me>
            <m>1 + x^2</m> doesn't possess any real zeroes. Does this mean that the power series for <m>f</m> converges everywhere? Let's see. Using the fact that <m>\frac{1}{1 - x} = 1 + x + x^2 + \ldots</m> for <m>\abs{x} \lt 1</m>, we can show that
            <me>
              \frac{1}{1 + x^2} = \frac{1}{1 - (-x^2)} = 1 + (-x^2) + (-x^2)^2 + (-x^2)^3 + \ldots = \sum_{n=0}^\infty (-1)^n x^{2n},
            </me>
            as long as <m>\abs{-x^2} \lt 1</m> which is just <m>\abs{x} \lt 1</m>. So even though <m>1 + x^2</m> has no real zeros, the power series for <m>f</m> only had radius of convergence <m>1</m>! What's going on?</p>

            <p>It turns out that when we're looking at quotients of analytic functions, we need to consider all of the zeros of the denominator, real and complex. For our example, <m>1 + x^2 = 0</m> has solutions <m>\pm i</m>, which are a distance of 1 away from the origin in the complex plane (0 on the real line). Since the closest zero to the center of our power series is 1 unit away, the radius of convergence is 1.</p>

            <theorem>
              <p>If <m>p(x), q(x)</m> are polynomials and <m>q(x_0) \neq 0</m>, then the radius of convergence of the power series representation is the distance from <m>x_0</m> to the closest root (real or complex) of <m>q</m>.
              </p>
            </theorem>

            <example><title>Radius of convergence for a rational function</title>
              <p> Compute the radius of convergence of the power series centered at <m>x = 0</m> of 
                <me>
                  f(x) = \frac{x^2}{(x +3)(x^2 + 3)}.
                </me>
              </p>

              <p>The zeroes of <m>q</m> are <m>x_1= -3, x_2 = i\sqrt{3}, x_3 = -i\sqrt{3}</m>. Using the distance formula, we can compute that the distances to the origin are <m>d_1 = 3, d_2 = \sqrt{3}, d_3 = \sqrt{3}</m>, and so the radius of convergence is <m>r = \sqrt{3}</m>.</p>
            </example>
          </subsection>


        </section>

        <section xml:id="ch-series-2">
            <title>Series solutions at ordinary points</title>
            <subsection>
              <title>Ordinary points</title>
              <p>Consider the second order linear differential equation
                <me>
                  y'' + p(x) y' + q(x) y = 0.
                </me>
              The fact that we have functions as <q>coefficients</q> for the terms means that our previous tools do not apply. Instead, we'll try to discover solutions using power series and recurrence relations. It will be much easier to do this if <m>p</m> and <m>q</m> are well-behaved where we center our power series.</p>

              <definition xml:id="def-ordinary-point">
                <p>A point <m>x = x_0</m> is an <term>ordinary point</term> for the differential equation
                <me>
                  y'' + p(x) y' + q(x) y = 0
                </me>
                if <m>p</m> and <m>q</m> are analytic at <m>x = x_0</m>. If a point is not ordinary, then it is called a <term>singular point</term> of the equation.</p>
              </definition>

              <example><title>Finding ordinary points</title>
                <p>Find the ordinary points of the differential equation
                  <me>
                    y'' + \frac{1}{x^2 - 1} y' + \frac{1}{x - 3} y = 0.
                  </me>
                </p>

                <p>Because <m>p, q</m> fail to be analytic at <m>x = \pm 1, 3</m> the equation is singular at those three points. Every other point is an ordinary point.</p>
              </example>
            </subsection>

            <subsection><title>Power series solutions at ordinary points</title>
              <p>We now give an example of solving a differential equation with series solutions. We'll start with a problem that we already know the solution to, before moving on to solving Airy's equation. It will be useful to recall the expressions for the derivatives of convergent power series. If <m>y = \ps</m>, then
              <md>
                <mrow>y' \amp= \sum_{n=1}^\infty n a_n x^{n-1},</mrow>
                <mrow>y'' \amp = \sum_{n=2}^\infty n(n-1) a_n x^{n-2}.</mrow>
              </md></p>

              <example><title>Finding series solutions</title>
                <p>Find the general solution to the differential equation
                  <m>y'' + y = 0</m>
                  using series methods.</p>

                <p>We expect to find two linearly independent solutions, and we expect the general solution to be of the form <m>y = c_1 y_1 + c_2 y_2</m> for arbitrary constants <m>c_1, c_2</m> because we have a second order equation. Notice that <m>p = 0</m> and <m>q = 1</m> which are analytic at every point, so we will work at the center <m>x = 0</m> and assume that our solution is of the form <m>y = \ps</m>.</p>

                <p>On taking derivatives and plugging into the differential equation, we get
                <me>
                  \sum_{n=2}^\infty n(n-1) a_n x^{n-2} + \ps = 0.
                </me>
                We want to combine these into a single sum and use a recurrence relation to find the solutions, but we need the terms and the index ranges to match. If we shift the index on the first sum by letting <m>n - 2 \to n </m>, we get
                <me>
                  \sum_{n=0}^\infty (n+2)(n+1) a_{n+2} x^{n} + \ps = 0.
                </me>
                Now we can combine the series to get
                <me>
                  \sum_{n=0}^\infty \left[(n+2)(n+1) a_{n+2} + a_n\right]x^n = 0
                </me>
                which gives the family of coefficient equations
                <me>
                  (n+2)(n+1)a_{n+2} + a_n = 0
                </me>
                which is convenient to rearrange as
                <me>
                  a_{n+2} = -\frac{1}{(n+2)(n+1)} a_n, \,\,\, n = 0, 1, 2, \ldots
                </me>
                </p>
                <p>Notice that we need two pieces of information to determine all of the information contained in this recursion: <m>n = 0</m> starts a chain that gives the coefficients for the even values of <m>n</m>, and <m>n = 1</m> starts a chain that gives that information for the odd values (corresponding to the two separate independent solutions). We consider each family separately.

                <md>
                  <mrow>n = 0\amp: a_2 = -\frac{1}{2 \cdot 1} a_0 = -\frac{1}{2!}a_0</mrow>
                  <mrow>n = 2\amp: a_4 = -\frac{1}{4 \cdot 3} a_2 = \frac{1}{4 \cdot 3 \cdot 2 \cdot 1} a_0 = \frac{1}{4!} a_0</mrow>
                  <mrow> n=4\amp: a_6 = -\frac{1}{6 \cdot 5} a_4 = -\frac{1}{6!} a_0.</mrow>
                </md>
                That is, the even terms seem to be of the form
                <m> a_{2k} = \frac{(-1)^k}{(2k!)} a_0</m>.</p>

                <p>For the odd values of <m>n</m>, we get
                <md>
                  <mrow>n = 1\amp: a_3 = -\frac{1}{3 \cdot 2} a_1 = -\frac{1}{3!}a_1</mrow>
                  <mrow>n = 3\amp: a_5 = -\frac{1}{5 \cdot 4} a_3 = \frac{1}{5 \cdot 4 \cdot 3 \cdot2 \cdot 1} a_1 = \frac{1}{5!} a_1</mrow>
                  <mrow> n=5\amp: a_7 = -\frac{1}{7 \cdot 6} a_5 = -\frac{1}{7!} a_1.</mrow>
                </md>
                which looks like <m>a_{2k+1} = \frac{(-1)^k}{(2k+1)!} a_1</m>. </p>

                <p>Now that we know the coeffients of the power series, we can write
                  <md>
                    <mrow>y \amp= a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \ldots</mrow>
                    <mrow>\amp= a_0 + a_1 x - \frac{1}{2!} a_0 x^2 - \frac{1}{3!} a_1 x^3 + \ldots</mrow>
                    <mrow>\amp = a_0 ( 1 - \frac{1}{2!} x^2 + \frac{1}{4!} x^4 - \ldots) + a_1 (x - \frac{1}{3!} x^3 + \frac{1}{5!}x^5 - \ldots)</mrow>
                  </md>
                  which allows us to write
                  <m>y = a_0 y_1 + a_0 y_2</m> where
                  <me>
                    y_1(x) = \sum_{n=0}^\infty \frac{(-1)^n}{(2n)!} x^{2n},  \hspace{1cm} y_2 = \sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)!} x^{2n+1}.</me>
                  </p>

                  <p>An easy application of the rato test shows that each of these series converges for all values of <m>x</m> (that is, the radius of converge is <m>\infty</m>).</p>

                  <p>Finally, we need to verify that these series are linearly independent. With two objects, this will be true as long as the functions aren't scalar multiples of each other, which is indeed the case.</p>

                  <p>(We might note that none of this is surprising in this case because the series in question have very nice closed forms you should recongize on sight!)</p>
              </example>

              <p>At an ordinary point, we will always follow the steps in the example above.
                <ol>
                  <li>Assume that a solution of the form <m>y = \ps</m> exists.</li>
                  <li>Plug the series into the equation to get a recurrence relation and find the coefficients in terms of <m>a_0</m> and <m>a_1</m>.</li>
                  <li>Use the ratio test to find the radius of convergence (which is where out solution is valid).</li>
                  <li>Verify that the solutions are linearly independent.</li>
                </ol></p>

                <theorem>
                  <p>Let <m>p, q</m> be analytic functions on some common interval <m>\abs{x - x_0} \lt r</m> centered at <m>x_0</m>. Then the general solution to the second order differential equation
                  <me>
                    y'' + p y' + q y = 0
                  </me>
                  can be represented as power series centered at <m>x_0</m> on the same interval. The coefficients in the series can be described in terms of the initial data <m>a_0, a_1</m>, and the general solution can be rearranged 
                  into the form
                  <me>
                    y = a_0 y_1 + a_1 y_2
                  </me>
                  where <m>y_1, y_2</m> are linearly independent.
                </p></theorem>

                <p>We conclude this chapter with the solution of Airy's equation.</p>

                <example><title>Airy's equation</title>
                  <p>Solve the equation 
                    <me>
                      y'' - x y = 0.
                    </me>
                  </p>

                  <p>Since <m>p = 0</m> and <m>q = x</m>, every point is an ordinary point. So we assume that a series solution exists of the form <m>y = \ps</m>. Plugging into the equation, we get
                  <me>
                    \sum_{n=2}^\infty n(n-1) a_n x^{n-2} - x \ps = 0.
                  </me>
                  Multiplying through, we get
                  <me>
                    \sum_{n=2}^\infty n(n-1) a_n x^{n-2} - \sum_{n = 0}^\infty a_n x^{n+1} = 0.
                  </me>
                  First, we'll make the powers match, then we'll shift indicies as necessary.
                  <me>
                    \sum_{n=0}^\infty (n+2)(n+1) a_{n+2} x^{n} - \sum_{n = 1}^\infty a_{n-1} x^{n} = 0.
                  </me>
                  Now that the powers of <m>x</m> match, the easiest way to make the index ranges match is to pop the first term off of the first sum.
                  <me>
                    2a_2 + \sum_{n=1}^\infty (n+2)(n+1) a_{n+2} x^{n} - \sum_{n = 1}^\infty a_{n-1} x^{n} = 0.
                  </me>
                  Before we combine, notice that right away we can see that <m>a_2 = 0</m>. This is going to have an interesting effect on the resulting series.
                  <me>
                    2a_2 + \sum_{n=1}^\infty \left[(n+2)(n+1) a_{n+2} - a_{n-1} \right] x^n = 0.
                  </me>
                  Setting the coefficients to be 0, we get the family of relations
                  <me>
                    a_{n+2} = \frac{1}{(n+2)(n+1)} a_{n-1}.
                  </me>
                  Because there is a separation of three index elements between the terms in the recursion, there will be three families of equations. Immediately, we can see that because <m>a_2 = 0</m>, the recursion imples that <m>a_5, a_8, a_{11}, \ldots = 0</m> as well. (From the theorem, we know that we should be able to get all the information we need in terms of <m>a_0</m> and <m>a_1</m>.)</p>

                  <p>For the set of coefficients beginning with <m>a_0</m>, we have

                  <md>
                    <mrow>n = 1\amp: a_3 = \frac{1}{3 \cdot 2} a_0</mrow>
                    <mrow>n = 4\amp: a_6 = \frac{1}{6 \cdot 5} a_3 = \frac{1}{6 \cdot 5 \cdot 3 \cdot 2} a_0</mrow>
                    <mrow> n=7\amp: a_9 = \frac{1}{9 \cdot 8} a_6 = \frac{1}{9 \cdot 8 \cdot 6 \cdot 5 \cdot 3 \cdot 2} a_0.</mrow>
                  </md>
                  There is a clear pattern, though it is annoying to write in closed form.</p>

                  <p>For the set of coefficients beginning with <m>a_1</m>, we have
                  <md>
                    <mrow>n = 2\amp: a_4 = \frac{1}{4 \cdot 3} a_1 </mrow>
                    <mrow>n = 5\amp: a_7 = \frac{1}{7 \cdot 6} a_4 = \frac{1}{7 \cdot 6 \cdot 4 \cdot3 } a_1 </mrow>
                    <mrow> n=8\amp: a_10 = \frac{1}{10 \cdot 9} a_7 = \frac{1}{10 \cdot 9 \cdot 7 \cdot 6 \cdot 4 \cdot 3} a_1.</mrow>
                  </md> 

                  Then the general solution to the equation is
                  <me> y = a_0 y_1(x) + a_1 y_2(x)</me>
                  where
                  <md>
                    <mrow>y_1(x) \amp= 1 +  \frac{1}{3 \cdot 2} x^3 + \frac{1}{6 \cdot 5 \cdot 3\cdot 2} x^6 + \ldots</mrow>
                    <mrow>y_2(x) \amp= x + \frac{1}{4 \cdot 3} x^4 + \frac{1}{7\cdot 6 \cdot 4 \cdot 3}x^7 + \ldots</mrow>
                  </md>
                  These power series are convergent everywhere, obviously linearly independent, and unfortunately have no closed form; that is, series representations is the best that we can do.</p>

                  <sage>
                    <input>
                      var( 'z y x'); y = 1 +  1/6 *x^3 + 1/180 *x^6
                      z = x + 1/12 *x^4 + 1/504 *x^7
                      a = plot([],title="(Approx) Solutions to Airy's equation")
                      a += plot(y, x, (-2,2), color='red')
                      a += plot(z, x, (-2,2))
                      show(a)
                    </input>
                  </sage>



                </example>
            </subsection>
        </section>

        <section xml:id="ch-series-3">
            <title>Legrendre series</title>
            <subsection><title>Legendre's equation</title>
            <p>Physics and applied mathematics provide a wealth of differential equations, some of which are sufficiently common to carry special names, notably
              <md>
                <mrow>(1 - x^2)y'' - 2x y' + c(c + 1)y = 0, \,\,\, \text{ (Legendre's equation)}</mrow>
                <mrow> y'' - 2xy' + 2cy = 0, \,\,\, \text{ (Hermite's equation)}</mrow>
                <mrow> (1 - x^2) y'' - xy' + c^2 y = 0 \,\,\, \text{ (Chebyshev's equation)}</mrow>
              </md>
              where <m>c</m> is some parameter that can take on any real value. All of these equations have an ordinary point at <m>x = 0</m>, and so have series solutions of the form <m>y = \ps</m>. The solutions to these equations for different values of <m>c</m> have special properties, particularly when <m>c</m> is an integer.</p>

              <p>In this section, we'll investigate the solutions and properties of the Legendre equation. First, note that the equation as written is not in our standard form. We'll need to rewrite it to get an estimate for the interval of convergence of our series solution (though it will be convenient to leave it in its original form when we attempt to solve it.) In the form
                <me>
                  y'' - \frac{2x}{1 - x^2} y' + \frac{c(c+1)}{1 - x^2} y = 0,
                </me>
                we can see that <m>p, q</m> are analytic at the very least on the interval <m>(-1,1)</m>, as the power series for <m>\frac{1}{1- x^2}</m> has radius of convergence <m>1</m>.</p>

                <p>Armed with the knowledge that our solutions exist, we can follow the usual technique to derive the solutions.</p>

                <exercise>
                  <p>Derive the general solutions to the Legendre equation using recurrence relations. 
                  </p>
                </exercise>

                <p>In general, the solutions to Legendre's equation for a fixed constant <m>c</m> are of the form
                <me>
                  y = y_1(x) + y_2(x)
                </me>
                with
                <md>
                  <mrow>y_1(x) \amp= a_0\left[1 - \frac{c(c+1)}{2}x^2 + \frac{(c-2)c(c+1)(c+3)}{4!} x^4\right. </mrow>
                  <mrow> \amp\left.\hspace{1in} - \frac{(c-4)(c-2)c(c+1)(c+3)(c+5)}{6!} x^6 +\ldots\right]</mrow>
                  <mrow></mrow>
                  <mrow>y_2(x) \amp= a_1\left[x - \frac{(c-1)(c+2)}{3!} x^3 + \frac{(c-3)(c-1)(c+2)(c+4)}{5!}x^5 + \ldots\right]</mrow>
                </md>
                </p>
              </subsection>

              <subsection>
                <title>Legendre polynomial</title>
                <p>It turns out that the most important case of the Legendre equation occurs when the constant <m>c</m> is an integer, which we emphasize by calling it <m>c=N</m>. In that case, the recursion that determines the solution to the equation is
                <me>
                  a_{n+2} = -\frac{n(n+1) - N(N+1)}{(n+1)(n+2)} a_n.
                </me>
                Since <m>N</m> is fixed, this means that <m>a_{N+2} = 0</m>, and moreover that the subsequent terms in the recursion are zero as well: <me>a_{N+2} = a_{N+4} = a_{N+6} = \ldots = 0.</me>
                That is, either <m>y_1</m> or <m>y_2</m> is a finite power series - a polynomial - depending on if <m>N</m> is even or odd. These polynomials turn out to be quite useful.
              </p>

              <definition>
                <p>Let <m>N</m> be a nonnegative intger. The <term>Legendre polynomial of degree <m>N</m></term>, denoted <m>P_N(x)</m>, is the polynomial solution to 
                <me>
                  (1- x^2) y'' - 2 x y' + N(N+1)y = 0,
                </me>
                normalized so that <m>P_N(1) = 1</m>.
                </p>
              </definition>

              <example>
                <p>The first few Legendre polynomials are easy to write down. Remember that each polynomial is normalized to have <m>P_N(1) = 1</m>.
                  <md>
                    <mrow>N \amp= 0:y_1(x) = a_0 \Rightarrow P_1(x) = 1.</mrow>
                    <mrow>N \amp= 1: y_2(x) = a_1 x \Rightarrow P_2(x) = x.</mrow>
                    <mrow>N \amp= 2: y_1(x) = a_0(1 - 3x^2) \Rightarrow P_2(x) = \frac{1}{2}(3x^2 -1).  </mrow>
                  </md>
                </p>
              </example>

              <p>For larger values of <m>N</m>, we need a better method to produce these polynomials. Fortunately for us, computer algebra systems have functions that produce these polynomials very quickly. (The following is in the mathematical language Sage. Similar functions exist in MATLAB, Mathematica, Octave, python, etc.)</p>

              <sage>
                <input>
                  var('x')
                  legendre_P(2,x)
                </input>
              </sage>

              <p>If we wish to proceeed by hand, there is an explicit formula for generating <m>P_N(x)</m> known as Rodrigues's formula:
                <me>
                  P_N(x) = \frac{1}{2^N N!} \frac{d^N}{dx^N} (x^2 - 1)^N, \hspace{1cm} N = 0, 1, 2, \ldots
                </me>
              </p>
              </subsection>

              <subsection>
                <title>Legendre polynomials and vector spaces</title>
                <p>Here we reach an instance of one of the most important larger themes of mathematical analysis - we can view functions with nice properties as vectors sitting inside an infinite dimensional vector space. With certain modifications, these spaces act very similarly to the familiar Euclidean spaces <m>\R^n</m>. One of the most important features of vector spaces is that vectors can be decomposed into orthogonal pieces with respect to some inner product in terms of a basis. For example, we can think of the Taylor series for a function as expressing that function in terms of the vectors <m>1, x, x^2, \ldots</m>, where the coefficients <m>\frac{f^{(n)}(a)}{n!}</m> are really the coordinates of the vector.</p>

                <p>The continuous functions on the interval <m>[-1,1]</m> are denoted <m>C^0[-1,1]</m>. (We're using symmetric intervals in this case because the Legendre polynomials are even or odd.) This space is a (infinite dimensional) inner product space with the dot product defined as
                <me>
                  f \cdot g = \ip{f}{g} = \int_{-1}^1 f(x) g(x) \, dx.
                </me>
                Following the analogy with <m>\R^n</m>, we define two vectors in an inner product space to be <term>orthogonal</term> if <m>\ip{f}{g} = 0</m>. Beyond a notion of <q>angle</q>, the inner product also gives a way to measure length with the <term>norm</term> defined in this case by
                <me>
                  \norm{f} = \sqrt{\ip{f}{f}} = \sqrt{\int_{-1}^1 f(x)^2 \, dx}.
                </me></p>

                <theorem>
                  <p>The Legendre polynomials are mutually orthogonal; that is,
                  <me>
                    \ip{P_j}{P_k} = \int_{-1}^1 P_j(x) P_k(x) \, dx = 0 \text{ if } j \neq k.
                  </me> 
                  Thus, the polynomials <m>P_0, P_1, \ldots, P_n</m> are an orthogonal basis for the space of polynomials of degree <m>n</m> or less.</p>
                </theorem>

                <proof>
                  <p>See Annin, Goode, p. 744.</p>
                </proof>

                <p>Furthermore, one can show that for a fixed integer <m>j</m> that
                <me>
                  \norm{P_j} = \sqrt{\frac{2}{2j+1}}.
                </me></p>

                <p>One of the most important properties of an orthgonal basis for an inner product space is that generic vectors can be decomposed into orthogonal pieces. Suppose that we have the set of Legendre polynomials <m>P_0, \ldots, P_N</m> as an orthogonal basis for the polynomials of degree <m>N</m> or less. Then we can compute the expansion of a polynomial in terms of Legendre polynomials by the formula
                <me>
                  p(x) = \sum_{i=0}^{N} a_i P_i = \sum_{i=0}^n \frac{\ip{p}{P_i}}{\norm{P_i}^2} P_i.
                </me>
                This should look familiar, as we are finding the coefficients <m>a_i</m> by using the vector projection formula <m>\proj_v u = \frac{u\cdot v}{\norm{v}^2} u</m>. Since we know that <m>\norm{P_i}^2 = \frac{2}{2i + 1}</m>, we get the specific expansion
                <me>
                  p(x) = \sum_{i=1}^N a_i P_i(x), \text{ where } a_i = \frac{2i+1}{2} \int_{-1}^1 p(x) P_i(x) \, dx.
                </me></p>

                <p>The real power of the expansion idea is that it holds for general functions, not just polynomials, under the additional assumption that <m>f \in C^1(-1,1)</m>; that is, both <m>f, f'</m> are continuous.</p>

                <theorem>
                  <p>Suppose that <m>f, f'</m> are continuous on <m>(-1,1)</m>. Then <m>f</m> has a Legendre series expansion on <m>(-1,1)</m> given by
                  <me>
                    f(x) = \sum_{i=1}^\infty a_i P_i(x)
                  </me>
                  where each <m>a_i</m> is given by the vector projection
                  <me>
                    a_i = \frac{\ip{f}{P_i}}{\norm{P_i}^2} = \frac{2i + 1}{2} \int_{-1}^1 f(x)P_i(x)\, dx.
                  </me></p>
                </theorem>

                <p>It is not at all obvious that an infinite series of functions should converge, nor what it means for a series of functions to converge to another function. Mathematicians wrestled with these ideas for more than a century establishing the basics of modern function theory. Proof of the theorem above will require you to consult an advanced textbook on spaces of functions. For now, accept the assertion that the theorem above does indeed have a proof and that mathematicians, physicists, and engineers have been using these sorts of ideas for far longer than they have been rigorously demonstrable.</p>

                <p>You might wonder why we would prefer to work with Legendre series rather than the Taylor series that are so easy to compute - first, notice that Taylor series only exist for <em>analytic</em> functions, where here we have Legendre series for functions that are merely continuously differentiable. Secondly, it turns out that working numerically with Taylor approximations is actually pretty terrible. The term used in numerical analysis is <term>ill-conditioned</term>. Legendre expansions are significantly more stable in computations. In many cases, Legendre series also converge to the function being approximated much more quickly than Taylor series, which we can illustrate with the following example.</p>

                <example><title>Taylor and Legendre approximation of <m>\sin \pi x</m>.</title>
                <p>The function <m>\sin \pi x</m> completes one period on the interval <m>(-1, 1)</m> and obviously has a continuous derivative. The Taylor expansion of <m>\sin \pi x</m> to degree 5 is given by the formula
                <me>
                  T_5(x) = \pi x - \frac{\pi^3 x^3}{6} + \frac{\pi^5 x^5}{120}
                </me>
                It is straightforward to compute the first few terms in the Legendre expansion as follows: First, note that since sine is odd, there will be no terms of even degree. That is, <m>\sin x = a_1 P_1(x) + a_3 P_3(x) + a_5 P_5(x)</m>.
                <md>
                  <mrow>
                  a_1 \amp= \frac{2(1) + 1}{2} \int_{-1}^1 x \sin \pi x\, dx = \frac{3}{\pi}</mrow>
                  <mrow>a_3 \amp= \frac{2(3) + 1}{2} \int_{-1}^1 \frac{1}{2}(3x^2 - 1) \sin\pi x \, dx = \frac{7}{\pi^3}(\pi^2 - 15)</mrow>
                  <mrow>a_5 \amp= \frac{2(5) + 1}{2} \int_{-1}^1 P_5(x) \, dx = \frac{11}{\pi^5} (945 - 105\pi^2 + \pi^4)</mrow>
                </md></p>

                <p>Let us compare these approximations.</p>
                <sage>
                  <input>
                   var ('x')
                    f(x) = sin(x)
                    g = plot(sin(pi*x), (-1,1))
                    P1 = legendre_P(1, x)
                    P3 = legendre_P(3, x)
                    P5 = legendre_P(5, x)
                    P(x) = 3/pi * P1 + 7/pi**3*(pi**2 - 15)*P3 + 11/pi**5*(945 - 105*pi**2 + pi**4)*P5
                    g+= plot(P(x), (-1,1), color='red')
                    T(x) = pi*x - pi**3*x**3/6 + pi**5*x**5/120
                    g+= plot(T(x), (-1,1), color = 'green')
                    show(g)
                  </input>
                </sage>

                <p>Notice how much deviation there is in the Taylor approximation towards the end of the interval, as compared to the Legendre approximation which even at 5th degree fits right into the sine function.</p>
                </example>

              </subsection>
        </section>

        <section xml:id="ch-series-4">
            <title>Series solutions at regular singularities</title>
            <subsection><title>Types of singular points</title>
            <p>So far, we've looked at differential equations at ordinary points, specifically
            <me>
              y'' + p(x)y' + q(x)y = 0.
            </me>
            That is, we've assumed that analytic solutions exist and proceeded with power series methods. However, a major area of interest in many applied settings is what happens at points where <p>p</p> and <m>q</m> fail to be analytic. These <term>singular points</term> are often very important to consider when analyzing a system. (You may need to know what happens to the behavior of the system as the input approaches a singular point.)</p>

            <definition xml:id="def-singular-point">
              <p>A point <m>x = x_0</m> is a <term>regular singular point</term> of the equation
              <me>
                y'' + p(x)y' + q(x) y= 0
              </me>
              if 
              <ol>
                <li><m>x_0</m> is a singular point of the equation,</li>
                <li>and both
                  <me>
                    \hat{p}(x) = (x - x_0)p(x), \hspace{1cm} \hat{q(x)}=(x-x_0)^2 q(x)
                  </me>
                  are analytic at <m>x = x_0</m>
                </li>
              </ol>
              A singular point that does not satisfy these conditions is called an <term>irregular singular point</term>.</p>
            </definition>

            <p>To give you some terminology (and a connection with complex analysis), a function <m>f(x)</m> for which <m>f(x)(x = x_0)^n</m> is analytic at <m>x_0</m> for <m>n = N</m> but singular at <m>x_0</m> for <m>n \lt N</m> is said to have a <term>pole of order <m>N</m> at <m>x_0</m></term>. The point of the definition above is that for a second order equation, essentially the worst singular behavior we can have and still work with series directly is second order poles. Similar statements apply to higher order equations.</p>

            <example><title>Classifying points</title>
              <p>Classify the points of the equation
                <me>
                  y'' + \frac{1}{x(x-1)^2}y' + \frac{x}{x(x-1)^3} y = 0
                </me>
              </p>

              <p>The equation clearly has singularities at <m>x = 0, 1</m>. For <m>x = 0</m>, 
              <me>
                \hat{p}(x) = x p(x) = \frac{1}{x - 1}, \hspace{1cm} \hat{q}(x) = xq(x) \frac{1}{(x-1)^3},
              </me>
              both of which are analytic at <m>x = 0</m>. Thus, <m>x =0</m> is a regular singular point.</p>
              <p>On the other hand, at <m>x = 1</m>, we have
              <me>
                \hat{p}(x) = (x - 1) p(x) = \frac{1}{x(x-1)},
              </me>
              which is singular at <m>x = 1</m>. Thus, <m>x =1</m> is an irregular singular point.</p>
            </example>
          </subsection>
          <subsection>
            <title>Series solutions at regular singular points</title>
            <p>Suppose that <m>x = x_0</m> is a regular singular point for the equation
            <me>
              y'' + p(x)y' + q(x)y = 0.
            </me>
            Essentially we can remove the singular behavior by multiplying through by <m>(x - x_0)^2</m> to get an equation of the form
            <me>
              (x - x_0)^2 y'' + (x-x_0)\hat{p}(x) y' + \hat{q}(x) y = 0
            </me>
            where <m>\hat{p} = (x-x_0)p(x), \hat{q} = (x - x_0)^2 q(x)</m> are analytic at <m>x_0</m>. Going one step further, we can make the substitution <m>u = x - x_0</m>, which moves the regular singular point to <m>x=0</m>, and so we can restrict our attention to equations of the form
            <me>
              x^2 + x p(x) y' + q(x) y = 0
            </me>
            where <m>p, q</m> are analytic at <m>0</m>. (We've essentially just pulled the singular behavior out of the coefficient functions to make it easier to analyze.)</p>

            <p>The most elementary equation in this family is the Cauchy-Euler equation, where <m>p, q</m> are just constants <m>p_0, q_0</m>. In that case, we have a second order linear constant coefficient homogeneous equation, which we can solve using the method of the indicial equation.</p>

            <theorem>
              <p>Suppose we have a Cauchy-Euler equation
              <me>
                x^2 y'' + x p_0 y' + q_0 y = 0.
              </me>
              The equation has solutions on <m>(0,\infty)</m> of the form <m>y = x^r</m>, where <m>r</m> is a solution to the indicial equation
              <men xml:id="eq-indicial">
                r(r-1) + p_0 r + q_0 = 0</men>
              </p>
            </theorem>

            <p>We can use the constant case to guide our method for more general coefficients. Suppose we have an equation with coefficient functions
              <me>
                x^2 y'' + xp(x)y' + q(x) y = 0
              </me>
              with <m>p,q</m> analytic at <m>0</m>. Then we can assume that there is some positive radius of convergence <m>R</m> for which <m>p, q</m> both have convergent series expansions
              <md>
                <mrow>p(x) \amp= p_0 + p_1 x + p_2 x^2 + \ldots</mrow>
                <mrow>q(x) \amp= q_0 + q_1 x + q_2 x^2 + \ldots</mrow>
              </md>
              We can plug these in to get the equation
              <me>
                x^2 y'' + x\left(p_0 + p_1 x + p_2 x^2 + \ldots\right)y' + \left(q_0 + q_1 x + q_2 x^2 + \ldots \right)y = 0.
              </me>
              Now we have to make a guess. Notice that if <m>x</m> is very small, then <m>p(x) \approx p_0</m> and <m>q(x) \approx q_0</m>, which means that near 0, the equation is approximately the Cauchy-Euler equation:
              <me>
               x^2 y'' + x\left(p_0 + p_1 x + p_2 x^2 + \ldots\right)y' + \left(q_0 + q_1 x + q_2 x^2 + \ldots \right)y \approx x^2 y'' + p_0 x y' + q_0 y = 0.
              </me>
              So we guess that the form of the series solution includes a term <m>x^r</m> that describes the behavior of the function near <m>x = 0</m> in <m>(0,R)</m> (where we're looking at a Cauchy-Euler equation) and a piece in terms of a power series that picks up the behavior away from <m>0</m>.
               </p>

               <p>That is, using this intuition, we can guess that for the equation
                <me>
                  x^2 y'' +  x p(x) y' + q(x) y = 0,
                </me>
                solutions will be of the form
                <men xml:id="eq-frob">
                  y(x) = x^r \sum_{n=0}^\infty a_n x^n, \,\,\,a_0 \neq 0
                </men> 
                where as before, <m>r</m> is a solution to the indicial equation
                <me>
                  r(r-1) + p_0 r + q_0 = 0.
                </me>
                This turns out to be the right idea, and series of the form <xref ref="eq-frob"/> are called <term>Frobenius series</term>.
              </p>

              <theorem xml:id="thm-frob-real-distinct">
                <p>For <m>x > 0</m>, suppose that
                <men xml:id="eq-reg-sing">
                  x^2 y'' + xp(x)y' + q(x)y = 0
                </men>
                and that <m>p, q</m> are analytic at 0 with a mutual radius of convergence <m>R</m>. Write <m>p(x) = \sum p_n x^n, q(x) = \sum q_n x^n</m>. Let <m>r_1, r_2</m> be roots of the indicial equation <xref ref="eq-indicial"/>, and assume that <m>r_1 \geq r_2</m> if the roots are real.</p>

                <p>Then <xref ref="eq-reg-sing"/> has a Frobenius series solution of the form
                <me>
                  y_1(x) = x^{r_1} \sum_{n=0}^\infty a_n x^n, \,\,\, a_0 \neq 0
                </me>
                which converges at least on <m>(0,R)</m>.</p>

                <p>If <m>r_1, r_2</m> are distinct and do not differ by an integer, then there exists a second linearly independent Frobenius series solution of the form
                <me>
                  y_2(x) = x^{r_2} \sum_{n=0}^\infty b_n x^n, \,\,\, b_0 \neq 0.
                </me>
                </p>
              </theorem>

              <p>The reason that we require the roots not to differ by an integer is that if they do, the resulting solutions will not be linearly independent. We'll address that case in the next section. First, let's look at an example of finding Frobenius series solutions. It is useful to note the following derivatives of Frobenius series:
                <me>
                  y(x) = x^r \sum_{n=0}^\infty a_n x^n = \sum_{n=0}^\infty a_n x^{n + r}
                </me>
                  
                <men xml:id="eq-frob-first">y'(x) = \sum_{n=0}^\infty (n+r) a_n x^{n + r - 1}</men>
                <men xml:id="eq-frob-second"> y''(x) = \sum_{n=0}^\infty (n+r)(n+r - 1) a_n x^{n + r - 2}.</men>
              </p>

              <example><title>Using recursion to find a Frobenius series</title>
              <p>Find two linearly independent solutions to the equation
                <men xml:id="ex-frob">
                  4x^2 y'' + 3 x y' + xy = 0.
                </men></p>

                <p>To check the form of the solutions, we'll put the problem into standard form - 
                  <me>
                    x^2 y'' + x (\frac{3}{4}) y' + x y = 0,
                  </me>
                  so that <m> p (x) = \frac{3}{4}, q(x) = x</m>, both of which are analytic on <m>(0 \infty)</m>. We need <m>p_0 = p(0) = \frac{3}{4},  q_0 = q(0) = 0</m> to find <m>r_1, r_2</m>.
                The indicial equation is therefore
                <me>
                  r(r-1) + \frac{3}{4} r + 0 = 0
                </me>
                which has roots <m>r = 0, r = \frac{1}{4}</m>. These are distinct real roots that do not differ by an integer, so we know that we have two linearly independent Frobenius solutions of the form
                <me>
                  y_1(x) = \sum_{n=0}^\infty a_n x^n, \,\,\, y_2(x) = x^{1/4} \sum_{n=0}^\infty b_n x^n.
                </me></p>

                <p>Plugging in the expressions for the series derivatives, we get
                <md>
                  <mrow>0 \amp = 4x^2 \sum_{n=0}^\infty (n+r)(n+r - 1) a_n x^{n + r - 2} + 3 x \sum_{n=0}^\infty (n+r) a_n x^{n + r - 1} + x \sum_{n=0}^\infty a_n x^{n + r}</mrow>
                  <mrow> \amp = \sum_{n=0}^\infty 4(n+r)(n+r -1) a_n x^{n + r} + \sum_{n=0}^\infty 3(n+r) a_n x^{n + r} +  \sum_{n=0}^\infty a_n x^{n + r + 1}</mrow>
                  <mrow> \amp = \sum_{n=0}^\infty 4(n+r)(n+r -1) a_{n - 1} x^{n + r} + \sum_{n=0}^\infty 3(n+r) a_n x^{n + r} +  \sum_{n=1}^\infty a_{n - 1} x^{n + r}</mrow>
                </md>
                To combine the series, we can pull off the term for <m>n=0</m> on the left sums and get the expression
                <me>
                x^r (4r(r-1) + 3r)a_0 = 0
                </me>
                which we should note leads directly to the indicial equation. For the remaining terms, we combine the series and get the recurrence relation
                <me>
                  [4(n + r)(n+r - 1) + 3(n+r)]a_n + a_{n-1} = 0
                </me>
                which simplifies to
                <me>
                  a_n = \frac{1}{4(n+r)(n+r - 1) + 3(n+ r)} a_{n-1}
                </me>
                Now we can use the values of <m>r</m> that we determined before to write the Frobenius series for each value.
                 </p>

                 <p><m>r = 0</m>: Here, we have a standard power series recursion
                 <me>
                  a_n = \frac{1}{4n(n-1) + 3n} a_{n-1} = \frac{1}{n(4n - 1)} a_{n-1}
                </me>
                which gives 
                <md>
                  <mrow>n \amp= 1: a_1 = \frac{1}{1 \cdot3 } a_0</mrow>
                  <mrow>n\amp =2: a_2 = \frac{1}{2 \cdot 7}a_1 = \frac{1}{1 \cdot 3 \cdot 2 \cdot 7}a_0 = \frac{1}{2! (3 \cdot 7)}a_0</mrow>
                  <mrow> n\amp = 3: a_3 = \frac{1}{3 \cdot 11} a_2 =  \frac{1}{3!(3 \cdot 7 \cdot 11)} a_0</mrow>
                </md>
                In general, this pattern looks something like
                <me>
                  a_n = \frac{1}{n! (3 \cdot 7 \cdot \ldots \cdot (4n -1))} a_0
                </me>
                Thus, the Frobenius series is 
                <me>
                  y_1(x) = a_0 x^0 \sum_{n=0}^\infty \frac{1}{n! (3 \cdot 7 \cdot \ldots \cdot (4n -1))} x^n, \,\,\, x \in (0, \infty).
                  </me> </p>

                  <p><m>r = \frac{1}{4}</m>: We'll use <m>b_i</m> to denote the coefficients of the second solution. Plugging in <m>r</m>, we get the recursion
                  <md>
                    <mrow>a_n \amp= \frac{1}{4(n+\frac{1}{4})(n+\frac{1}{4} - 1) + 3(n+ \frac{1}{4})} a_{n-1}</mrow>
                    <mrow>\amp= \frac{1}{n(4n+1)} a_{n-1}</mrow>
                  </md>
                  which will give
                  <me>
                    a_n = \frac{1}{n!(5 \cdot 9 \cdot \ldots \cdot (4n + 1))} a_0.
                  </me>
                  Thus, a second independent solution is 
                  <me>
                    y_2(x) = b_0 x^{1/4} \sum_{n=1}^\infty \frac{1}{n!(5 \cdot 9 \cdot \ldots \cdot (4n + 1))} x^n, \,\,\, x \in (0\infty).
                  </me>
                  We could choose any value for <m>a_0, b_0</m> and have a solution to <xref ref="ex-frob"/>, so it is convenient to let both be 1, and then write
                  <md>
                    <mrow>y_1(x) \amp= \sum_{n=0}^\infty \frac{1}{n! (3 \cdot 7 \cdot \ldots \cdot(4n -1))} x^n, \,\,\, x \in (0, \infty)</mrow>  
                    <mrow> y_2(x) \amp = x^{1/4} \sum_{n=1}^\infty \frac{1}{n!(5 \cdot 9 \cdot \ldots \cdot (4n + 1))} x^n, \,\,\, x \in (0,\infty)</mrow>
                  </md></p>

              </example>



          </subsection>
        </section>

        <section xml:id="ch-series-5">
            <title>More on Frobenius methods (partial)</title>
            <subsection><title>Frobeius theory at regular singular points</title>

            <p>More may end up here eventually. For now, it is sufficient to note that the method of reduction of order can be used to produce a second linearly independent solution from the first Frobenius series obtained. The form of the resulting answer will vary depending on the nature of the roots.</p> 
            <theorem xml:id="thm-frob-general">
              <p>For <m>x > 0</m>, suppose that
              <men xml:id="eq-reg-sing-2">
                x^2 y'' + xp(x)y' + q(x)y = 0
              </men>
              and that <m>p, q</m> are analytic at 0 with a mutual radius of convergence <m>R</m>. Write <m>p(x) = \sum p_n x^n, q(x) = \sum q_n x^n</m>. Let <m>r_1, r_2</m> be roots of the indicial equation <xref ref="eq-indicial"/>, and assume that <m>r_1 \geq r_2</m> if the roots are real.</p>

              <p>Then <xref ref="eq-reg-sing-2"/> has two linearly independent solutions on at least the interval <m>(0,R)</m>. Exactly one of the following cases holds.
              <ol>
                <li><m>r_1 - r_2</m> is not an integer.
                <md>
                    <mrow>y_1(x) = x^{r_1} \sum_{n=0}^\infty a_n x^n, \,\,\, a_0 \neq 0</mrow>
                    <mrow>y_2(x) = x_{r_2} \sum_{n=0}^\infty b_n x^n, \,\,\, a_0 \neq 0.</mrow>
                  </md>
                </li>
                <li><m>r_1 = r_2 = r</m>. 
                <md>
                  <mrow>y_1(x) = x^{r} \sum_{n=0}^\infty a_n x^n, \,\,\, a_0 \neq 0</mrow>
                  <mrow>y_2(x) = y_1(x) \ln x + x_{r} \sum_{n=0}^\infty b_n x^n, \,\,\, a_0 \neq 0.</mrow>
                </md>
                </li>
                <li><m>r_1 - r_2</m> a positive integer.
                <md>
                  <mrow>y_1(x) = x^{r_1} \sum_{n=0}^\infty a_n x^n, \,\,\, a_0 \neq 0</mrow>
                  <mrow>y_2(x) = Ay_1(x) \ln x + x_{r_2} \sum_{n=0}^\infty b_n x^n, \,\,\, a_0 \neq 0.</mrow></md></li>
              </ol>
              </p>
            </theorem>
          </subsection>
          <subsection><title>Irregular points</title>
          <p>Irregular points are important but outside the scope of this class. See <url href="https://ocw.mit.edu/courses/mathematics/18-305-advanced-analytic-methods-in-science-and-engineering-fall-2004/lecture-notes/eight1.pdf">this lecture</url> for example as a starting point. The main idea is that solutions still involve Frobenius series, but now multiplied by exponential factors. That is, irregular points give rise to solutions that blow up or collapse!</p>
        </subsection>

        </section>

        <section xml:id="ch-series-6">
          <title>Bessel equations</title>
          <subsection><title>Bessel's differential equation</title>
            <p>One of the most important differential equations in applied mathematics and physical science is the <url href="https://en.wikipedia.org/wiki/Bessel_function">Bessel differential equation</url>, which has the form
            <men xml:id="eq-bessel">
              x^2 y''+ x y' + (x^2 - \alpha^2) y = 0
            </men>
            for some complex constant <m>\alpha</m>, which is called the <term>order</term> of the solutions arising from the associated equation. Bessel's equation and the associated solutions are a core technique in the analysis of the propagation of waves, for example.</p>

            <p>In general, we have to write the solutions to this equation in terms of integrals; that is, there are not closed form solutions. But since <xref ref="eq-bessel"/> has a regular singular point at <m>0</m>, in the case where <m>\alpha</m> is a nonnegative real constant, we can use Frobenius techniques to write series solutions.</p>

            <p>Since <m>p(0) = 1</m> and <m>q(0) = -\alpha^2</m>, we get the indicial equation
            <me>
              r(r-1) + r - \alpha^2,
            </me>
            which has solutions <m>\pm \alpha</m>. As long as <m>2\alpha</m> is not an integer, we can find two Frobenius series solutions of the form
            <me>
              y(x) = x^r \sum_{n=0}^\infty a_n x^n,
            </me>
            and again it is useful to recall the derivative formulas <xref ref="eq-frob-first"/> and <xref ref="eq-frob-second"/>.</p>

            <p>We proceed with our standard technique and recover a series solution from a recurrence relation. Substituting into the <xref ref="eq-bessel"/>, we get
            <me>
              \sum_{n=0}^\infty [(r + n)^2 - \alpha^2]a_n x^{n+r} + \sum_{n=2}^\infty a_{n-2} x^{n+r} = 0.
            </me>
            We can peel off the first two terms of the left sum. For <m>n=0</m>, we get the indicial equation. For <m>n = 1</m>, we get
            <me>
              [(r + 1)^2 - \alpha^2]a_1 = 0.
            </me>
            For <m>n \geq 2</m>, we get the recurrence relation
            <me>
              [(n+r)^2 - \alpha^2]a_n = a_{n -2}.
            </me></p>

            <p>To construct the first solution, consider the root <m>r = \alpha</m>. The recurrence relation becomes
            <me>
              a_n = -\frac{1}{n(2\alpha + n)} a_{n-2}.
            </me>
            Since the gap between the terms in the relation is two, there will be two families of coefficients. Since <m>a_1 = 0</m>, it must be that <m>a_3, a_5, \ldots = 0</m>. On the other hand, with a little work we get that the even terms are characterized by
            <me>
              a_{2k} = \frac{(-1)^k}{2 \cdot 4 \ldots \cdot (2k) \cdot (2\alpha + 2) \ldots \cdot (2 \alpha + 2k)} a_0,
            </me>
            which is a bit of a bear. Next, we'll try to reduce this into a more palatable form. First, we can pull out a whole pile of 2s to get something a bit more condensed, and a form that might help guide a useful choice for <m>a_0</m>.
            <me>
              a_{2k} = \frac{(-1)^k}{2^{2k} k! (\alpha + 1) \cdots (\alpha + k)} a_0, k = 1, 2, \ldots.
            </me>
            Then we have a Frobenius series solution to <xref ref="eq-bessel"/> of the form
            <men xml:id="eq-bessel-series">
              y_1(x) = a_0 x^\alpha \left[1 + \frac{(-1)^k}{2^{2k} k! (\alpha + 1) \cdots (\alpha + k)} x^{2k}\right].
              </men></p>

          </subsection>

          <subsection><title>Bessel functions of the first kind</title>
            <p>When <m>\alpha = N</m> is an integer, the formula collapses even further if we make a clever choice for the arbitrary constant <m>a_0</m>. Let
            <me>
              a_0 = \frac{!}{N!2^N}.
            </me>
            Then the solution is denoted <m>J_N(x)</m> and is called the <term>Bessel function of the first kind of order <m>N</m></term> and has the form
            <me>
              J_N(x) = \sum_{k=0}^\infty \frac{(-1)^k}{k!(N + k)!} \left(\frac{x}{2}\right)^{2k + N}.
            </me></p>

            <p>The following code uses prebuilt libraries to construct and plot the first few examples of <m>J_N</m></p>

            <sage>
              <input>
                var('x')
                g = plot(bessel_J(0, x), (0,50), color = 'red')
                g += plot(bessel_J(1, x), (0,50), color = 'blue')
                g += plot(bessel_J(2,x), (0,50), color = 'green')
                show(g)
              </input>
            </sage>

            <p>The graphs above should suggest <em>why</em> these functions are useful at modeling wave propagation - each function oscillates with decreasing amplitude and has an infinite number of zeros on <m>(0,\infty)</m>.</p>

            <p>To deal with the case where <m>\alpha \in (0,\infty)</m> is not an integer, we need to introduce a generalization of the factorial function called the <term><url href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</url></term>, one of the most famous special functions in mathematics. The basic idea of the gamma function is to <q>fill in the spaces</q> between the factorials with a nice, smooth function that retains the special properties of the factorial function (that is, a nice function <m>y</m> for which <m>y(x) = x y(x-1)</m>). It turns out that the best way to do that is with a function defined in terms of an integral.</p>

            <definition xml:id="def-gamma-function">
              <p>The gamma function <m>\Gamma: (0, \infty) \to \R</m> is defined by the integral formula
              <men xml:id="eq-gamma-prop">
                \Gamma(x) = \int_0^\infty t^{x-1} e^{-t} \, dt.
              </men>
              </p>
            </definition>

            <p>The gamma function is actually defined on all complex numbers outside of the negative integers, but we do not need that fact here. </p>

            <theorem xml:id="thm-gamma-factorial">
              <p>For <m>x > 0</m>, 
              <me>
                \Gamma(x + 1) = (x + 1) \Gamma(x).
              </me>
              </p>
            </theorem>

            <proof>
              <p>This is an immediate consequence of integration by parts.
                <md>
                  <mrow>\Gamma(x + 1) \amp= \int_0^\infty t^{x} e^{-t} \, dt</mrow>
                  <mrow> \amp= \left[-t^{x} e^{-t}\right]_0^\infty + x \int_0^\infty t^{x - 1} e^{-t} \, dt</mrow>
                  <mrow>\amp= x \Gamma(x)</mrow>.
                </md>
              </p>
            </proof>

            <p>If we had a starting point, we could show that the gamma function generates the factorials. Fortunately, it is very easy to show that
              <me>
                \Gamma(1) = \int_0^\infty e^{-t} \, dt = 1.
              </me>
              Then induction on <xref ref="eq-gamma-prop"/> gets us
              <me>
                \Gamma(2) = 2\Gamma(1) = 2!, \,\,\, \Gamma(3) = 3\Gamma(2) = 3!,
              </me>
              and in general that for a positive integer <m>N</m> that
              <me>
                \Gamma(N) = N!.
              </me>
            </p>

            <p>At the same time, we can get the factorial-like formula
              <men xml:id="eq-gamma-factorial-like">
                \Gamma(x + k + 1) = [(x+1)(x+2)\cdots(x+k)] \Gamma(x).
              </men></p>

            <p>Now we can extend the gamma function to negative numbers that aren't integers by using the relation
              <men xml:id="eq-gamma-extension">
                \Gamma(x) = \frac{\Gamma(x+1)}{x}
              </men>
            If <m>x \in (-1,0)</m>, then <m>x + 1 \in (0,1)</m>, and so for <m>x \in (-1,0)</m> we use <xref ref="eq-gamma-extension"/> as the definition of <m>\Gamma(x)</m>. We can continue in this way inductively: if <m>x \in (-2,-1)</m>, then <m>x+ 1 \in (-1,0)</m>, which is now defined, and again we use formula <xref ref="eq-gamma-extension"/>. The resulting function is continuous off the negative integers and has the graph below.</p>

            <sage>
              <input>
                var('x')
                plot(gamma(x), (-5,5), detect_poles='true', ymin=-5, ymax = 5)
              </input>
            </sage>

            <p>The gamma function allows us to define the Bessel functions for non-integer orders. First, choose
            <me>
              a_0 = \frac{1}{2^\alpha \Gamma(\alpha + 1)}
            </me>
            and plug into <xref ref="eq-bessel-series"/> to get
            <men xml:id="eq-bessel-first">
              J_\alpha(x) = \sum_{k=0}^\infty \frac{(-1)^k}{\Gamma(k+1) \, \Gamma(\alpha + k + 1)} \left(\frac{x}{2}\right)^{2k + \alpha},
            </men>
            which by the definition of <m>\Gamma</m> reduces to <m>J_N(x)</m> when <m>N</m> is an integer.</p>
          </subsection>

          <subsection><title>Bessel functions of the second kind</title>

          <p>Because the differential equation <xref ref="eq-bessel"/> has lots of solutions, there are many ways to write down a pair of linearly independent functions that solve the equation. The function that we will consider here, we choose for convenience by convention of physicists and engineers, but there are several equivalent ways of formulating the general solution. First, note that as long as <m>2\alpha</m> isn't an integer, the second root to the indicial equation, <m>r = -\alpha</m> will also have an associated and linearly independent solution. It is straightforward that we can just replace <m>\alpha</m> with <m>-\alpha</m> in <xref ref="eq-bessel-first"/> to get
          <me>
            J_{-\alpha} = \sum_{k=0}^\infty \frac{(-1)^k}{\Gamma(k + 1) \, \Gamma(k - \alpha + 1)} \left(\frac{x}{2}\right)^{2k - \alpha}
          </me>
          and a general solution
          <me>
            y(x) = c_1 J_\alpha(x) + c_2 J_{-\alpha}(x).
          </me>
          In practice, we don't use <m>J_{-\alpha}</m> as the second solution, but rather a linear combination of <m>J_{\alpha}</m> and <m>J_{-\alpha}</m>. Thus we define the <term>Bessel function of the second kind of order <m>\alpha</m></term> by 
          <me>
            Y_\alpha(x) = \frac{J_\alpha(x) \cos \alpha \pi - J_{-\alpha}(x)}{\sin \alpha \pi}.</me>
            The function is construced this way because it extends naturally to the case where <m>\alpha</m> is an integer, avoiding the complications of the reduction of order technique needed to derive the second solution from the first in the <m>\alpha</m> is an integer case. Without doing the computations, it is enough to know that if <m>\alpha = n</m>, then we can define
            <me>
              Y_n(x) = \lim_{\alpha \to n} \left[\frac{J_\alpha(x) \cos \alpha \pi - J_{-\alpha}(x)}{\sin \alpha \pi}\right]
            </me>
            and that this sum can be computed.</p>

            <p>Thus the general solution to Bessel's equation is
              <me>
                y(x) = c_1 J_\alpha(x) + c_2 Y_\alpha(x)</me>
              for <m>\alpha \in (0,\infty)</m>.
            </p>

            <sage>
              <input>
                var('x')
                g = plot(bessel_Y(0, x), (0,50), color = 'red')
                g += plot(bessel_Y(1, x), (0,50), color = 'blue')
                g += plot(bessel_Y(2,x), (0,50), color = 'green')
                show(g)
              </input>
            </sage>

            <p>Notice that the <m>Y_\alpha</m> are unbounded at <m>x = 0</m>, which is typically not a desired characteristic in a physical problem.</p>
          </subsection>

          <subsection><title>Bessel-Fourier series</title>
          <p>We've already seen that Legendre polynomials from an orthgonal basis for the <m>C^1</m> functions on a finite interval with respect to the inner product
          <me>
            \ip{f}{g} = \int f(x) g(x) \, dx.
          </me>
          It turns out that Bessel functions can be used to form a different orthogonal system of functions that serve as a basis for the <m>C^1</m> functions (one of of an infinite family), but with a slightly different inner product. In fact, just one function <m>J_\alpha</m> can be used to generate the basis (where the choice of <m>\alpha</m> lets us describe functions in terms of a solution to a particular physical scenario).</p>

          <p>The graphs of <m>J_\alpha</m> should be evidence that for a given <m>\alpha</m>, <m>J_\alpha</m> has infinite zeros (as it wiggles back and forth across the <m>x</m>-axis). For a fixed <m>\alpha</m>, let <m>\la_{\alpha, n}</m> denote the sequence of positive zeros of <m>J_\alpha</m>.</p>

          <p>To construct an orthonormal family, we'll need to use a modification of the standard inner product by introducing a <term>weight function</term>. One way of thinking about the standard inner product <m>x \cdot y = \sum x_i y_i</m> or <m>\ip{f}{g} = \int fg</m> is that we give all terms equal importance. But there's no reason that we have to do that. A weight function <m>w(x) \geq 0</m> essentially turns an inner product into a weighted average instead of a standard average. On <m>\R^n</m>, a weighted inner product has the form <m>x \cdot y = \sum x_i y_i w(i)</m>. On function spaces, we have the <term>weighted inner product with respect to <m>w</m></term> which is given by
          <me>
            \ip{f}{g} = \int_0^1 f(x)g(x)w(x)\, dx.
          </me>
          We do this because we change the geomerty of the space when we use a new inner product. New vectors will be orthogonal. We will choose the weight function <m>w(x) = x</m>, and we get the following fact about the family <m>J_\alpha(\la_{\alpha,n}x)</m>.</p>

          <proposition>
            <p>Let <m>\alpha</m> be a fixed parameter, and let <m>\la_{\alpha, n}</m> denote the sequence of positive zeros of the Bessel function of the first kind <m>J_\alpha</m>.</p>

            <p>Then with respect to the weighted inner product
              <me>
                \ip{f}{g} = \int_0^1 xf(x)g(x)\, dx
              </me>
              the family of functions given by
              <me>
                \{J_\alpha(\la_{\alpha,n} x)\}_{n=1}^\infty
              </me>
              is an orthogonal set of vectors in <m>C^1[0,1]</m>.
            </p>
          </proposition>

          <p>What we're really saying here is that for any <m>n\neq m</m> that
          <me>
            \ip{J_\alpha(\la_{\alpha,n}x)}{J_\alpha(\la_{\alpha,m}x)} = \int_0^1 x J_\alpha(\la_{\alpha,n}) J_\alpha(\la_{\alpha,m}x) \, dx = 0.
          </me>
          We can also show (with quite a bit of elbow grease) that 
          <me>
            \norm{J_\alpha(\la_{\alpha,n}x)} = \frac{1}{2} \left[ J_{\alpha + 1}(\la_{\alpha,n})\right]^2.
          </me></p>

          <p>Even better, the family <m>J_\alpha(\la_{\alpha,n}x)</m> turns out to be a basis for <m>C^1</m>, and so we can express any <m>C^1</m> function in terms of a series of Bessel functions.</p>

          <theorem xml:id="thm-fourier-bessel"><title>Fourier-Bessel expansion</title>
            <p>Suppose that <m>f \in C^1[0,1]</m>. Then for <m>x \in [0,1]</m>,
            <me>
              f(x) = a_1 J_\alpha(\la_{\alpha,1}x) + a_2 J_\alpha(\la_{\alpha,2}x) + \ldots = \sum_{n = 1}^\infty a_n J_\alpha (\la_{\alpha, n} x),
            </me>
            where the coefficients <m>a_i</m> are given by the projection formula
            <me>
              a_i = \frac{\ip{f}{J_{\alpha}(\la_{\alpha,i}x)}}{\norm{J_{\alpha}(\la_{\alpha,i}x)}^2} = \frac{2}{\left[ J_{\alpha + 1}(\la_{\alpha,i})\right]^2} \int_0^1 x f(x) J_\alpha(\la_{\alpha, i}x)\, dx.
              </me></p>
          </theorem>

          <p>Again, the proof of convergence of these types of series is beyond the scope of the course and more suited to a course in real analysis or partial differential equations. We keep emphasizing the vector notation not just to impress with our ability to assemble huge blocks of mathematics, but to point out that all we're really doing here is linear algebra with fancier vectors. </p>
        </subsection>

        </section>


        <section xml:id="ch-series-7">
            <title>Series as vectors (an introduction to <m>L^2</m>)</title>
            <p>One of the major themes of this section is that functions are secretly vectors. Even better, functions with some level of regularity are vectors in an inner product space, which means that we can use basic ideas from linear algebra to understand them (once we know which space they are in). The key idea in all of this is that the integral is really just an infinite sum indexed by the input <m>x</m>, and so the dot product is really just a finite dimensional version of an integral (in fact, there are areas of math where this is made explicit, as in probability theory). </p>

            <p>With Legendre polynomials and Bessel functions, we've demonstrated that <m>C^1</m> functions live inside an infinite dimensional inner product space. Just as in finite dimensional linear algebra, we used the orthogonal expansion theorem to write <m>f</m> as
            <me>
              f(x) = \sum \frac{\ip{f(x)}{b_n(x)}}{\ip{b_n(x)}{b_n(x)}} b_n(x)
            </me>
            for some orthogonal basis <m>b_n</m>. We pick a set of basis function based on the particular problem we are trying to understand. For example, if we're doing numerical estimation of a function, we can choose <m>b_n</m> to be the Legendre polynomials and get better approximations that the basic Taylor series. If we're studying functions acting in a system associated with a Bessel equation, we can express the function in terms of the solutions to that system to understand how the wave behavior is driving the function.</p>

            <p>The theoretical backdrop for all of this is the theory of <term>Hilbert space</term>, which is where questions of convergence are first addessed. One of the most important Hilbert spaces is called <m>L^2[a,b]</m> - these are the functions <m>f</m> defined on an interval <m>[a,b]</m> with finite energy. A function is in <m>L^2</m> if 
            <me>
              \sqrt{\int_a^b f(x)^2 \, dx} \lt \infty
            </me>
            (where <m>dx</m> is a more powerful type of differential called <term>Lebesgue measure</term>). But this is precisely the inner product we considered with the Legendre polynomials:
            <me>
              f \cdot g = \ip{f}{g} = \sqrt{\int_a^b f(x)g(x) \, dx}.
            </me></p>

            <p>Taylor polynomials and Legendre polynomials are outstanding bases to use for <m>L^2</m> if we're interested in looking at local behavior of a function (say zeros or extreme points). But lots of other functions live in <m>L^2</m> that don't have nice behavior that we might be interested in looking at. For example, we might have an equation that models a circuit, and we want to feed in a square wave to see what the response might be.</p>

            <sage>
              <input>
                var('x')
                f1(x) = 1
                f2(x) = 0
                f(x) = piecewise([((0,1),f1),((1,2),f2), ((2,3), f1)])
                plot(f, (0,3))
              </input>
            </sage>

            <p>Now, we can come up with a Legendre polynomial expansion for this function (at least on the piecewise bits), but it isn't going to really capture what's interesting about the square wave - a square wave is <term>periodic</term>. If only there was a basis of functions for <m>L^2</m> that was orthogonal and periodic and that could be used to model functions like the square wave. Such a basis would be extremely useful for solving differential equations involving oscillations. If only....
          </p>
        </section>
    </chapter>

    <chapter>
      <title>All things Fourier</title>
      <section>
        <title>Periodicity</title>
        <p>As hinted at in the prior section, our concern for this chapter will be to study functions that are <term>periodic</term>, that is, functions that repeat their behavior after a fixed change in the input variable (called the <term>period</term> of the function). Periodic functions are some of the most important in the physical sciences, engineering, and mathematics. We're setting the stage for your eventual adventures in partial differential equations, and so we need to treat two kinds of input variables, space and time. A typical function we might consider could look something like <m>f(x, t) = y</m>, where the position <m>x</m> and time <m>t</m> both determine the behavior of the output <m>y</m>.</p>

        <p>For now, we'll restrict our attention to functions of one variable. A function <m>f(t)</m> is <term>periodic of period <m>T</m></term> if there is some constant <m>T</m> so that <me>f(t) = f(t + T)</me> for all <m>t</m>. Of course, if one such <m>T</m> exists, many do. That is, any integer multiple of the period is also a period.  (For example, <m>\cos t = \cos (t + 2n\pi)</m> for every integer <m>n</m>.) The smallest such <m>T</m> for which <m>f(t) = f(t + T)</m> is called the fundamental period.</p>

        <p>One iteration of the function over a period is called a <term>cycle</term>. The number of cycles per unit input is referred to in earlier courses as the frequency of the functions, but this isn't really a correct idea unless we're talking about a simple sine or cosine. Consider the function
        <me>
          f(t) = \cos 2\pi t + \frac{2}{3} \cos 6\pi t.
        </me>
        The periods of the individual summands are 1 and 1/3 but the period of the entire function is 1.</p>

        <sage>
          <input>
            var('t')
            f(t) = cos(2*pi*t) + 2/3*cos(6*pi*t)
            plot(f, (-3,3))
          </input>
        </sage>

        <p>While the period of <m>f</m> is 1, it doesn't really make sense to talk about the <em>frequency</em> of <m>f</m> as a single number, because <m>f</m> seems to carry two frequencies, one from each of the constituent cosines. Naturally, we might be led to the naive conclusion that any sum of period functions is periodic, but this can't be true. Functions with irrational periods make this impossible. Consider the function
        <m> g(t) = \cos t + \sin \sqrt{2} t</m>. No integer multiple of <m>\sqrt{2}</m> will ever coincide with an integer multiple of <m>1</m>, and so these functions will never coincide at the end of a cycle. (In fact, the graph of this function is quite chaotic.)</p>

        <sage>
          <input>
            var('t')
            f(t) = cos(sqrt(2)*t) + cos(t)
            plot(f, (-60,60))
          </input>
        </sage>

        <p>On the other hand, it's pretty easy to show that any two functions with rational periods must eventually coincide.</p>

        <exercise>
          Suppose that <m>f</m> has rational period <m>T</m> and <m>g</m> has rational period <m>S</m>. Find the period of the function <m>f + g</m>.
        </exercise>

        <p>The classical example we see over and over again of time periodic systems is <em>harmonic oscillation</em>. We discover that the state of a harmonic oscillator is given by a function
        <me>
          x(t) = A \sin(2 \pi \nu t + \phi)
        </me>
        for amplitude <m>A</m>, frequency <m>\nu</m>, and phase <m>\phi</m>. The period of the function is <m>1/\nu</m> (which you should show by direct computation).</p>

        <p>For spatial periodicity, the classical problem is the distribution of heat on a circular ring. A point on the ring is designated by an angle <m>\theta</m>. Space periodicity comes from the fact that for any function that depends on the position on the ring <m>f(\theta)</m>, it will be the case that <m>f(\theta) = f(\theta + 2\pi)</m>. The problem is to give some initial distribution of heat on the ring. In the long run as <m>t\to\infty</m>, we expect the ring to be evenly distributed (constant for all <m>\theta</m>). So how can we write down the short term distributions?</p>

        <p>Fourier's idea was to try to model the answer with a sum of sines:
          <me>T(\theta, t) = \sum_{n=1}^\infty A_n(t) \sin(n \theta + \phi_n).</me>
          This idea basically launched the development of the language of modern physics and engineering.</p>

        <p>A term <m>\sin(n\theta + \phi)</m> is called a <term>harmonic</term> (from the mathematics of music). In the same way that a musical chord is made up of several harmonics combined into a whole, so too is the function <m>T(\theta,t)</m> made up of harmonics that build the complete function. As weird as it seems, in some sense we're going to treat physical systems like music and try to pull the solutions apart in terms of their consituent notes.</p>
      </section>
      <section>
        <title>Complex exponentials and trig functions</title>
        <subsection><title>Complex numbers and arithmetic</title>
        <p>I am a complex analyst by trade, so I will assert that the complex numbers are really the only way to go if you want to do mathematical analysis. The use of complex numbers and functions to represent real systems is common convention for several reasons. First, the math is WAY easier in the complex setting. Basically, without delving into complex analysis, we're going to be cheating by thinking of functions as complex-valued with real inputs.</p>

        <p>First and foremost, I am a mathematician, not an engineer, so I will use the symbol <m>i</m> to represent the terribly named imaginary unit
        <me>
          i = \sqrt{-1}.
        </me>
        Feel free to use <m>j</m> if it makes your brain happy.</p>

        <p>A complex number is a quantity
          <me>z = x + iy</me>
          where <m>x, y</m> are real numbers. <m>x</m> is called the <term>real part</term> of <m>z</m> and is written <m>x = \RE z</m>. Likewise, <m>y</m> is called the <term>imaginary part</term> of <m>z</m> (yes, the imaginary part is real) and is written <m>y = \IM z</m>. Notice that this means that we can think of the real numbers as living inside the complex numbers (it's the set with <m>y = 0</m>).</p>

          <p>You should think of the complex numbers as essentially having the same structural features as the real numbers. The operations are defined on the real and imaginary parts, with the additional feature that <m>i^2 = -1</m>. Say that <m>z = x + iy</m> and <m>w = a + ib</m>.
          <ul>
          <li>Addition: <me>z + w =  (x + a) + i(y + b)</me></li>
          <li>Multiplication <me>zw = (a + ib)(x + iy) = (ax - by) + i(bx + ay)</me></li>
          <li>Division: <me>\frac{z}{w} = \frac{x + iy}{a + ib} = \frac{(x + iy)(a - ib)}{(a + ib)(a - ib)} = \frac{(ax + by)}{a^2 + b^2} + i \frac{ay - bx}{a^2 + b^2}</me></li>
          </ul></p>

          <p>If <m>z = x + iy</m> then the <term>complex conjugate</term> of <m>z</m> is
          <me>
            \cc{z} = x - iy.
          </me>
          Complex conjugates follow the following <q>distribution</q> like rules:
          <me>
            \cc{z + w} = \cc{z} + \cc{w}, \,\, \cc{zw} = \cc{z} \cc{w}, \,\, \cc{\cc{z}} =z.
          </me>
          With complex conjugates, it is very easy to write formulas for the real and imaginary part of a complex number.
          <me>
            \RE z = \frac{z + \cc{z}}{2}, \,\,\, \IM z = \frac{z - \cc{z}}{2i}
          </me></p>

          <p>Complex conjugates are very important in calculus and matrix settings, so we have an alternative notation that we apply to more complicated objects: <m>\cc{z} = z\ad</m>. Because integrals are limits of finite sums, integration respects complex conjugation and we write
          <me>
            \left(\int f(t)g(t) \, dt \right)\ad = \int f(t)\ad g(t)\ad \, dt.
          </me></p>

          <p>A natural way to think of a complex number is as a vector, with the real part representing the <q><m>x</m>-coordinate</q> and the imaginary part the <q><m>y</m></q>. Because of the way that complex conjugates are defined, we can compute the norm or magnitude of a complex number by
          <me>
            \abs{z} = \sqrt{x^2 + y^2} = \sqrt{z \cc{z}}.
          </me>
          In practice, it is useful to write the above expression as <m>\abs{z}^2 = z \cc{z}</m>.</p>

          <theorem><title>Parallelogram identity</title>
          <p>Let <m>z, w</m> be complex numbers. Then
          <me>
            \abs{z + w}^2 + \abs{z - w}^2 = 2(\abs{z}^2 + \abs{w}^2).
          </me></p>
        </theorem>

        <p>Another useful related property is
        <me>
          \abs{z + w}^2 = \abs{z}^2 + 2\RE z\cc{w} + \abs{w}^2.
        </me></p>

        <p>Finally, just as a vector in <m>\R^2</m> can be thought of in both Cartesian and polar forms, so too can a complex number. Given <m>z = x + iy</m>, the length of the vector can be written <m>r = \abs{z} = \sqrt{x^2 + y^2}</m> and the angle from the positive real axis satisifies <m>\tan \theta = \frac{y}{x}</m>. The angle <m>\theta</m> is called the <term>argument</term> of the complex number and is sometimes written <m>\arg z</m>. First note that this means that the complex numbers of radius 1 coincide precisely with the unit circle. Since <m>x = r \cos \theta</m> and <m>y = r \sin \theta</m>, we can write
        <me>
          z = r\cos\theta + i r \sin \theta,
        </me>
        though we're quickly going to get a more useful form. The thing to note is that complex numbers naturally encode and support the trig functions that represent rotations. As complicated as complex numbers might seem, this is the major theme of our use of them: <em>complex numbers are rotations of real numbers</em>.</p>
      </subsection>

        <subsection><title>Complex exponentials and Euler's formula</title>
        <p>The most important complex function is the complex exponential <m>f(z) = e^z</m>.  Given a Taylor series with a positive interval of convergence, we can extend the Taylor series formula to complex numbers simply by plugging in complex numbers. The set of convergence will be a disk with diameter equivalent to the real interval of convergence. Hence, we make the definition
        <me>
          e^z = \sum_{n=0}^\infty \frac{z^n}{n!},
        </me>
        which converges for all <m>z \in \C</m>. It is an easy exercise (do it!) to show that <me>\cc{e^z} = e^{\cc{z}}.</me></p>

        <p>As is the case in one variable, <m>e^z</m> is the unique function that is its own derivative. We will take on authority that the complex exponential satisfies the usual algebraic properties (as defining the complex logarithm is more appropriately done in a complex analysis course):
        <md>
          <mrow> e^{z + w} \amp= e^z + e^w</mrow>
          <mrow> {e^z}^w \amp e^{zw}</mrow>
        </md></p>

        <p>Using the properties of exponential functions, we can write
          <me>
            e^z = e^{x + iy} = e^x e^{iy}.
          </me>
          This doesn't seem like we've gained much, but something magical happens with <m>e^{iy}</m>. When the power of a complex exponential is purely imaginary, it corresponds to a point on the unit circle! That is,
          <men xml:id="eq-cis">
            e^{iy} = \cos y + i \sin y,
          </men>
          which is an almost unimaginably useful formula called <term>Euler's formula</term>. You might suspect such a relationship should hold given that the Taylor series for sine and cosine <em>almost</em> look like they add to <m>e^x</m> in the real case, but the alternating signs are wrong. In the complex case, this relation holds.</p>

        <p>What this means is that complex multiplication and division (and hence exponentiation) can be rewritten in form that is tractable. If <m>z_1 = r_1 e^{i\theta_1}, z_2 = r_2 e^{i\theta_2}</m>, then
        <me>
          z_1 z_2 = r_1 r_2 e^{i(\theta_1 + \theta_2)},
        </me>
        and
        <me> \frac{z_1}{z_2} = \frac{r_1}{r_2} e^{i \theta_1 - \theta_2}.</me>
        It's easy to miss how weird and useful this idea is. Essentially, we've come up with a way to multiply vectors and produce a vector. </p>

        <p>Since <xref ref="eq-cis"/> expresses <m>e^{i\theta}</m> in terms of sine and cosine, we can reverse this relation to define sine and cosine in terms of complex exponentials.
        <me> \cos \theta = \frac{e^{i\theta} + e^{-i\theta}}{2}, \,\,\, \sin \theta = \frac{e^{i\theta} - e^{-i\theta}}{2i}.</me></p>

        <p>One of the themes of this course is that we can decompose functions into pieces that have useful properties. One way that you might not have seen before is to pull a function apart into symmetric pieces. A function is called <term>even</term> if <m>f(x) = f(-x)</m> (corresponding to symmetry about the <m>y</m>-axis. A function is called <term>odd</term> if <m>f(x) = -f(-x)</m> (corresponding to symmetry about the origin). We can build even and odd functions out of any function in the following way:
        <me>
          f_{even}(x) = \frac{f(x) + f(-x)}{2}, \,\,\, f_{odd}(x) = \frac{f(x) - f(-x)}{2}.
        </me></p>

        <exercise>
          Show that <m>f_{even}</m> is even and that <m>f_{odd}</m> is odd, justifying the notation.
        </exercise>

        <p><m>f_{even}</m> and <m>f_{odd}</m> decompose <em>any</em> function into symmertic pieces, as
        <me>
          f_{even}(x) + f_{odd}(x) = \frac{f(x) + f(-x)}{2} + \frac{f(x) - f(-x)}{2} = f(x).
        </me>
        It turns out that the even and odd pieces of the complex exponential are the trig functions.
        <me>
          \frac{e^{i\theta} + e^{-i\theta}}{2} = \cos \theta,
        </me>
        and
        <me>
          \frac{e^{i\theta} - e^{-i\theta}}{2} = i \sin \theta.
        </me></p>
      </subsection>

      <subsection><title>What complex exponentials mean and how to use them</title>

        <p>Typically, we're going to be working with a real parameter <m>t</m>. So define the function
          <me>
            f(t) = e^{it} = \cos t + i \sin t.
          </me>
        As <m>t</m> moves along in time, what is <m>f</m> doing? If we think about the complex numbers as vectors (that is, <m>f(t) = (\RE f(t), \IM f(t))</m>, we can think of <m>f</m> as a parametric function where <m>x(t) = \cos t</m> and <m>y(t) = \sin t</m>. Then as <m>t</m> increases, the graph of <m>f</m> traces out the unit circle, rotating in the counterclockwise direction. Likewise, negative complex exponentials trace out the unit circle but in the clockwise direction. One complete traversal of the circle takes place as <m>t</m> goes from <m>0</m> to <m>2 \pi</m>.</p>

        <p>It is typical practice in applications to work with integer values for frequency, and so in engineering (and general Fourier analysis), the convention is to use as the basic exponential relation
          <me>
            e^{2\pi i t} = \cos 2\pi t + i \sin 2\pi t,
          </me>
        as this gives a function with frequency <m>1</m> (that is, we complete one loop around the unit circle for <m>t \in [0,1]</m>). In other words, <m>e^{2\pi i t}</m> has frequency <m>1 \hz</m>. More generally, 
        <me>
          e^{2\pi n i t} = \cos{2 \pi n t} + i \sin 2\pi n t</me>
        has frequency <m>n \hz</m>. </p>

        <p>Notice that <m>e^{-2\pi n i t}</m> also has frequency <m>n \hz</m>, but travels in the opposite direction. This will be an important theme in Fourier analysis - oscillations of a given frequency have <em>two</em> components, propagating in opposite directions.</p>

        <p>Complex exponentials are vastly easier to work with than the sinusoids. Recall that the generic sine function is
          <me>
            f(t) = A \sin(2 \pi \nu t + \phi)
          </me>
        with frequency <m>\nu</m>, amplitude <m>A</m>, and phase <m>\phi</m>. The complex exponential with the same information is
          <me>
            g(t) = A e^{i(2\pi \nu t + \phi)}
          </me>
          which can be split into
          <me>
            g(t) = Ae^{i\phi} e^{2 \pi \nu u t}.
          </me>
        Notice that
        <me>
          \abs{Ae^{i\phi} e^{2 \pi \nu u t}} = \abs{A} = A
        </me>
        and that <m>e^{i\phi}</m> describes a starting point on the unit circle.
        </p>

        <p>Finally, we'll come to a fact that you may have seen in previous courses, though usually without explanation. 
          <fact>
            The sum of two sinusoidal functions with the same frequency is a single sinusoidal function of that frequency.
          </fact>
        This is not at all obvious from a superimposed graph.</p>

        <sage>
          <input>
            var('x')
            f(x) = 2*sin(3*pi*x + pi/2)
            g(x) = sin(3*pi*x)
            pic = plot(f, color='red')
            pic += plot(g, color = 'green')
            show(pic)
          </input>
        </sage>

        <p>But if we plot the sum, we get:</p>

        <sage>
          <input>
            var('x')
            f(x) = 2*sin(3*pi*x + pi/2)
            g(x) = sin(3*pi*x)
            pic = plot(f + g, color='blue')
            show(pic)
          </input>
        </sage>

        <p>Here the computation is massively simplified by the use of complex exponentials. Consider
          <me>
            A_1 e^{i(2 \pi \nu + \phi_1)} + A_2 e^{i(2 \pi \nu + \phi_2)}.
          </me>
          Using basic exponential algebra, we can compute
          <md>
            <mrow>\amp A_1 e^{i(2 \pi \nu + \phi_1)} + A_2 e^{i(2 \pi \nu + \phi_2)}</mrow>
            <mrow> \amp = e^{2 \pi i \nu t}\left(A_1 e^{i\phi_1} + A_2 e^{i\phi_2}\right).</mrow>
          </md>
          That is, the sum of two exponentials with the same frequency is another exponential of the same frequency (but with a new phase and amplitude).</p>
      </subsection>
      </section>
      <section>
        <title>Sums of periodic functions</title>
      </section>
      <section>
        <title>Questions of convergence</title>
      </section>
      <section>
        <title>First applications</title>
      </section>
      <section>
        <title>Fourier series and linear algebra</title>
      </section>
      <section>
        <title>The Fourier transform</title>
      </section>
      <section>
        <title>Convolution</title>
      </section>
      <section>
        <title>Applications of convolution</title>
      </section>

    </chapter>

    <chapter xml:id="ch-laplace">
      <title>The Laplace Transform</title>
      <section xml:id="ch-laplace-1">
        <title>Motivation for transform methods</title>
        <p>The most important differential equations are <term>linear constant coefficient differential equations</term>, usually introduced in the second order. For example,
        <me> y'' + 3 y' + 2 y = \cos x</me>
        is such an equation. More generally, these equations have the form 
        <me> a_n y^{(n)}(x) + \ldots + a_1 y'(x) + a_0 y(x) = f(x).</me> (This can be written <me>\mathcal{F}(y) = f(x)</me> where <m>\mathcal{F}</m> is the operator that takes <m>y</m> to the differential expression on the left hand side.)
        These equations are important because they are the general equations that describe harmonic motions - vibration, oscillation, and rotations.</p>

        <p>The method of solution we develop in earlier courses is to find the solution by working in steps. First, we solve the associated <term>homogeneous equation</term> in order to identify any functions that are sent to the zero function by the differential operator. This is called the <term>homogeneous solution</term> <m>y_h</m>. To do so, we use the method of the characteristic equation or the method of annihilators. In either case, we have to factor potentially high degree polynomials.</p>

        <example>
            <p>To solve the homogeneous equation
                <me>
                y'' + 3y' + 2y = 0,
                </me>
            we first consider the characteristic equation
                <me>
                    m^2 + 3m + 2 = 0
                </me>
            which factors as 
                <me>
                    (m+2)(m+1) = 0
                </me>
            with solutions <m>m = -2, m=-1</m>. Then the principle of superposition tells us that the homogeneous solution is
                <me>
                    y_h = c_1 e^{-2x} + c_2 e^{-x}.
                </me></p>
        </example>

        <p>Now, for the second step, we need to find a <term>particular solution</term> that we can feed into the differential equation so that <m>\mathcal{F}(y_p) = f(x)</m>. We learn several methods for this in first courses that consider differential equations, usually including the method of undetermined coefficients and the method of variation of parameters. In both cases, we're limited to either forcing functions nice enough that we can guess the form of the solution (in the case of undetermined coefficients) or functions that we can integrate (in the case of variation of parameters). However, in physical systems we often want to consider driving or forcing functions <m>f(x)</m> that represent physical conditions that don't have nice integrals: for example, switches that turn on and off, or impulses that are applied to the system instantaneously (such as a strike). While the methods above can be adapted to deal with these, it is easier to <term>transform</term> the problem into a different mathematical setup that will simplify the process of dealing with these common physical situations.</p>

        <p>The model problem to keep in mind is the following mass-spring type setup:
            <me>
                y'' + ay ' + by = F
            </me>
            where <m>F</m> might switch on and off or act on the system instantaneously (and so cannot be a continuous function). By transforming the problem into an equivalent domain, we can deal with these fundamental situations very easily.
        </p>
        </section>

        <section xml:id="ch-laplace-2">
            <title>The Laplace Transform</title>
            <subsection><title>Definition of the Laplace Transform</title>
            <p>Our first transform method is the <term>Laplace transform</term>, which takes a function <m>f:[0,\infty) \to \R</m> and produces a function of a new variable <m>F(s)</m>.</p>
            <definition xml:id="def-laplace">
                <p>Let <m>f</m> be a function on <m>[0,\infty)</m>. The Laplace transform of <m>f</m> is defined by the integral operator
                <me>
                    \mathcal{L}[f] = \int_0^\infty f(t) e^{-st} \, dt = F(s).
                </me></p>
            </definition>

            <p>Before we get into when we can use the Laplace transform and why it is so powerful, we'll start with some examples of computing the transforms of some basic functions.</p>

            <example>
              <title><m>f(t) = 1</m></title>
                <p>We'll start by computing the Laplace transform of <m>f(t) = 1</m>. Recall that when we're dealing with improper integrals (with limits at <m>\infty</m> in this case), we need to consider the integral as shorthand for a limit.</p>

                <md>
                <mrow>L[1] \amp = \int_0^\infty 1 \cdot e^{-st} \, dt = \lim_{B \to \infty} \int_0^B e^{-st} \, dt</mrow>
                <mrow>\amp = \lim_{B \to \infty} \left(\frac{-1}{s} e^{-st}\right) \bigg\rvert_0^B</mrow>
                <mrow>\amp = \frac{-1}{s} \lim_{B\to\infty} e^{-sB} - 1</mrow>
                <mrow>\amp = \frac{1}{s}.</mrow>
                </md>

                <p>So <m>\mathcal{L}[1] = \frac{1}{s}</m>, as long as <m>s >0</m> (or the improper integral fails to converge, a condition that will usually apply).</p>
            </example>
            <example>
              <title><m>f(t) = t</m></title>
              <p>Now let's look at <m>f(t) = t</m>, which will require integration by parts.

              <md>
                <mrow>\mathcal{L}{t} \amp= \int_0^\infty t \cdot e^{-st} \, dt = \lim_{B \to \infty} \int_0^B t e^{-st} \, dt</mrow>
                <mrow>\amp= \lim_{B \to \infty} \frac{-t}{s} e^{-st}\bigg\rvert_0^B - \int_0^B \frac{-1}{s}e^{-st}\, dt</mrow>
                <mrow>\amp = \lim_{B \to \infty} \frac{-B}{s} e^{-sB} - 0 + \frac{1}{s} \lim_{B \to \infty} \int_0^B e^{-st}\, dt</mrow>
                <mrow>\amp= 0 - 0 + \frac{1}{s}\mathcal{L}[1] = \frac{1}{s^2}</mrow>
              </md>
              so long as <m>s > 0</m>.
              (Note that it is very easy so show that as <m>B \to \infty</m>, we get <m>\frac{-B}{s}e^{-sB} \to 0</m> by L'Hospital's rule.)
              </p>
            </example>
            <exercise>
              <p>Use the same idea as the previous example to show that 
                <me>
                \mathcal{L}[t^n] = \frac{n!}{s^{n+1}}, \hspace{1cm} s > 0</me>
                for any positive integer <m>n</m>.
              </p>
            </exercise>
            <example>
              <title><m>f(t) = e^{at}</m></title>
              <p>
                Now consider the exponential function <m>e^{at}</m> for some constant <m>a</m>.

              <md>
                <mrow>\mathcal{L}[e^{at}] \amp= \int_0^\infty e^{at} \cdot e^{-st} \, dt = \lim_{B \to \infty} \int_0^B e^{-(s - a)t} \, dt</mrow>
                <mrow>\amp = \lim_{B\to\infty} \frac{-1}{s-a} e^{-(s-a)t} \bigg\rvert_0^B</mrow>
                <mrow>\amp= \frac{-1}{s - a} \lim_{B \to \infty} e^{-(s-a)B} - 1 = \frac{1}{s-a}</mrow>
              </md>
              as long as <m> s > a</m>.
              </p>
            </example>
            <example>
              <title><m>f(t) = \cos bt</m></title>
              <p>Our final introductory example concerns the basic trig function <m>f(t) = \cos bt</m>. (We can use parts here or just note that
              <me>
                \int e^{at}\cos{bt} \, dt= \frac{e^{at}}{a^2 + b^2}(a\cos bt + b \sin bt)
              </me>
              which is a standard table integral.) Then,

              <md>
                <mrow>\mathcal{L}[\cos bt] \amp= \int_0^\infty \cos bt \cdot e^{-st} \, dt = \lim_{B \to \infty} \int_0^B e^{-st} \cos bt  \, dt</mrow>
                <mrow>\amp= \lim_{B \to \infty} \left[ \frac{e^{-st}}{s^2 + b^2} (-s\cos bt + b \sin bt)\right]\bigg\rvert_0^B</mrow>
                <mrow>\amp= \frac{s}{s^2 + b^2}</mrow>
                as long as <m>s > 0</m> to ensure that the improper integral converges.
              </md></p>
            </example>
            <exercise>
              <p>Use a standard integral and the argument in the previous example to show that 
                <me>
                  \mathcal{L}[\sin bt] = \frac{b}{s^2 + b^2}, \hspace{1cm} s > 0.
                </me>
              </p>
            </exercise>
            Notice that at this point, we have constructed the Laplace transforms of the basic forms of forcing functions that we consider in the method of undetermined coefficients (which is no accident!).
          </subsection>

          <subsection><title>Basic properties of the Laplace transform</title>
          <p>One of the most important properties of the integral is that it acts linearly on the integrand. That is, for constants <m>a,b</m> and integrable functions <m>f,g</m>, we have
            <me>
            \int af + bg = a\int f + b \int g.
             </me>
             Because the integral is linear, often operations that are defined in terms of integrals are also linear. This is the case with the Laplace transform as long as it converges, since 
             <md>
              <mrow>\mathcal{L}[af(t) + bg(t)] \amp= \int_0^\infty e^{-st}(a f(t) + b g(t)) \, dt</mrow>
              <mrow>\amp= a \int_0^\infty e^{-st}f(t) \, dt + b \int_0^\infty e^{-st} g(t) \, dt</mrow>
              <mrow>\amp = a \mathcal{L}[f(t)] + b \mathcal{L}[g(t)]</mrow>
              </md>
              This often gets recorded as the equivalent conditions
              <men>
                \mathcal{L}[f+g] = \mathcal{L}[f] + \mathcal{L}[g]
              </men>
              and
              <men>
                \mathcal{L}[cf] = c \mathcal{L}[f].
              </men>
           </p>
           <example><title>Laplace transform of a combination of functions</title>
            <p>Compute the Laplace transform of <m>f(t) = t^2 + 3e^{2t} + \sin 3t</m>, including the domain.</p>

            <p>Linearity makes this easy, since
              <md>
                <mrow>\mathcal{L}[2t^2 + 3e^{2t} + \sin 3t] \amp= \mathcal{L}[2t^2] + \mathcal{L}[3e^{2t}] + \mathcal{L}[\sin 3t]</mrow>
                <mrow>\amp= 2\mathcal{L}[t^2] + 3\mathcal{L}[e^{2t}] + \mathcal{L}[\sin 3t]</mrow>
                <mrow>\amp= 2\frac{1}{s^2} + 3\frac{1}{s-2} + \frac{3}{s^2 + 9}, \hspace{1in} s > 2.</mrow>
              </md>
            </p>
          </example>
           <p>Combined with the results from the previous discussion, we can now compute the transforms of a lot of the most important forcing functions that we consider when we learn constant coefficient linear equations. So far, it seems like we haven't gained much of an advantage, but the next section should make clear that the Laplace transform can handle functions far beyond what can be done with earlier methods. Also still open is the big question:
            <question>
              <p>How does the Laplace transform apply to differential equations?</p>
            </question></p>
         </subsection>

         <subsection><title>Piecewise continuous functions</title>
          <p>The most important quality of the Laplace transform is that it can be used on functions that are piecewise continuous. In physical situations, this correponds to forcing functions that change at points of time. For example, a continuously applied constant force could suddenly become a harmonic force modeled by a trig function at some time, and then at a later time, the force could disappear. Functions that change definitions, but which remain continuous in between those points of change are called <term>piecewise continuous functions</term>.</p>

          <definition xml:id="def-piecewise">
            <p>A function <m>f(t)</m> is a piecewise continuous function on an interval <m>[a,b]</m> if the interval can be divided up into finitely many subintervals <m>[a_i,a_{i+1}]</m> so that 
            <ol>
              <li><m>f</m> is a continuous function when restricted to each <m>[a_i,a_{i+1}]</m>, and</li>
              <li><m>f</m> does not possess a  vertical asymptote at any of the points <m>a_i</m> (that is, <m>f</m> approaches a finite limit at the end of each subinterval)</li>
            </ol>
            If <m>f</m> is piecewise continuous on every interval of the form <m>[0,b]</m> for any constant <m>b</m>, then <m>f</m> is piecewise continuous on <m>[0,\infty)</m>.</p>
          </definition>
        </subsection>

        </section>

        <section xml:id="ch-laplace-3">
          <title>Periodic functions</title>
        </section>

        <section xml:id="ch-laplace-4">
          <title>The Laplace transform of derivatives</title>
        </section>
        <section xml:id="ch-laplace-5">
          <title>The shifting theorems</title>
        </section>
        <section xml:id="ch-laplace-6">
          <title>Heaviside functions</title>
        </section>
        <section xml:id="ch-laplace-7">
          <title>Impulses - the Dirac delta <q>function</q></title>
        </section>
        <section xml:id="ch-laplace-8">
          <title>Convolution</title>
        </section>


    </chapter>



</book>

</pretext>
