<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2021-01-05T15:17:00-08:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Linear algebra</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script src="https://sagecell.sagemath.org/embedded_sagecell.js"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", "AMScd.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script xmlns:svg="http://www.w3.org/2000/svg">sagecellEvalName='Evaluate (Sage)';
</script><link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/colors_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link xmlns:svg="http://www.w3.org/2000/svg" href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div xmlns:svg="http://www.w3.org/2000/svg" class="hidden-content" style="display:none">\(\DeclareMathOperator{\RE}{Re}
  \DeclareMathOperator{\IM}{Im}
  \DeclareMathOperator{\ess}{ess}
  \DeclareMathOperator{\intr}{int}
  \DeclareMathOperator{\dist}{dist}
  \DeclareMathOperator{\dom}{dom}
  \DeclareMathOperator{\diag}{diag}
  \DeclareMathOperator{\span}{span}
  \DeclareMathOperator{\null}{null}
  \DeclareMathOperator{\rank}{rank}
  \DeclareMathOperator{\col}{col}
  \DeclareMathOperator{\row}{row}
  \DeclareMathOperator{\proj}{proj}
  \DeclareMathOperator\re{\mathrm {Re~}}
  \DeclareMathOperator\im{\mathrm {Im~}}
  
  \newcommand\dd{\mathrm d}
  \newcommand{\eps}{\varepsilon}
  \newcommand{\To}{\longrightarrow}
  \newcommand{\hilbert}{\mathcal{H}}
  \newcommand{\s}{\mathcal{S}_2}
  \newcommand{\A}{\mathcal{A}}
  \newcommand\h{\mathcal{H}}
  \newcommand{\J}{\mathcal{J}}
  \newcommand{\M}{\mathcal{M}}
  \newcommand{\F}{\mathbb{F}}
  \newcommand{\N}{\mathcal{N}}
  \newcommand{\T}{\mathbb{T}}
  \newcommand{\W}{\mathcal{W}}
  \newcommand{\X}{\mathcal{X}}
  \newcommand{\D}{\mathbb{D}}
  \newcommand{\C}{\mathbb{C}}
  \newcommand{\BOP}{\mathbf{B}}
  \newcommand{\Z}{\mathbb{Z}}
  \newcommand{\BH}{\mathbf{B}(\mathcal{H})}
  \newcommand{\KH}{\mathcal{K}(\mathcal{H})}
  \newcommand{\pick}{\mathcal{P}_2}
  \newcommand{\schur}{\mathcal{S}_2}
  \newcommand{\R}{\mathbb{R}}
  \newcommand{\Complex}{\mathbb{C}}
  \newcommand{\Field}{\mathbb{F}}
  \newcommand{\RPlus}{\Real^{+}}
  \newcommand{\Polar}{\mathcal{P}_{\s}}
  \newcommand{\Poly}{\mathcal{P}(E)}
  \newcommand{\EssD}{\mathcal{D}}
  \newcommand{\Lop}{\mathcal{L}}
  \newcommand{\cc}[1]{\overline{#1}}
  \newcommand{\abs}[1]{\left\vert#1\right\vert}
  \newcommand{\set}[1]{\left\{#1\right\}}
  \newcommand{\seq}[1]{\left\lt#1\right&gt;}
  \newcommand{\norm}[1]{\left\Vert#1\right\Vert}
  \newcommand{\essnorm}[1]{\norm{#1}_{\ess}}
  \newcommand{\tr}{\operatorname{tr}}
  \newcommand{\ran}[1]{\operatorname{ran}#1}
  \newcommand{\nt}{\stackrel{\mathrm {nt}}{\to}}
  \newcommand{\pnt}{\xrightarrow{pnt}}
  \newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}
  \newcommand{\ad}{^\ast}
  \newcommand{\inv}{^{-1}}
  \newcommand{\adinv}{^{\ast -1}}
  \newcommand{\invad}{^{-1 \ast}}
  \newcommand\Pick{\mathcal P}
  \newcommand\Ha{\mathbb{H}}
  \newcommand{\HH}{\Ha\times\Ha}
  \newcommand\Htau{\mathbb{H}(\tau)}
  \newcommand{\vp}{\varphi}
  \newcommand{\ph}{\varphi}
  \newcommand\al{\alpha}
  \newcommand\ga{\gamma}
  \newcommand\de{\delta}
  \newcommand\ep{\varepsilon}
  \newcommand\la{\lambda}
  \newcommand\up{\upsilon}
  \newcommand\si{\sigma}
  \newcommand\beq{\begin{equation}}
  \newcommand\ds{\displaystyle}
  \newcommand\eeq{\end{equation}}
  \newcommand\df{\stackrel{\rm def}{=}}
  \newcommand\ii{\mathrm i}
  \newcommand{\vectwo}[2]
  {
     \begin{pmatrix} #1 \\ #2 \end{pmatrix}
  }
  \newcommand{\vecthree}[3]
  {
     \begin{pmatrix} #1 \\ #2 \\ #3 \end{pmatrix}
  }
  \newcommand\blue{\color{blue}}
  \newcommand\black{\color{black}}
  \newcommand\red{\color{red}}
  
  \newcommand\nn{\nonumber}
  \newcommand\bbm{\begin{bmatrix}}
  \newcommand\ebm{\end{bmatrix}}
  \newcommand\bpm{\begin{pmatrix}}
  \newcommand\epm{\end{pmatrix}}
  \numberwithin{equation}{section}
  \newcommand\nin{\noindent}
  \newcommand{\nCr}[2]{\,_{#1}C_{#2}} 
  \newcommand{\vec}[1]{{\bf #1}}
  \newcommand{\ps}{\displaystyle \sum_{n=0}^\infty a_n x^n}
  \newcommand{\psg}{\displaystyle \sum_{n=0}^\infty b_n x^n}
  \newcommand{\hz}{\,\mathrm{Hz}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="linanal.html"><span class="title">Math 344 - Linear Analysis II Notes</span></a></h1>
<p class="byline">Ryan Tully-Doyle</p>
</div>
</div></div>
<nav xmlns:svg="http://www.w3.org/2000/svg" id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3"><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="section-2.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="ch-0.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="section-4.html" title="Next">Next</a></span></div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="section-2.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="ch-0.html" title="Up">Up</a><a class="next-button button toolbar-item" href="section-4.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div xmlns:svg="http://www.w3.org/2000/svg" id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter"><a href="frontmatter-1.html" data-scroll="frontmatter-1"><span class="title">Front Matter</span></a></li>
<li class="link">
<a href="ch-0.html" data-scroll="ch-0"><span class="codenumber">1</span> <span class="title">What you should know</span></a><ul>
<li><a href="section-1.html" data-scroll="section-1">Introduction</a></li>
<li><a href="section-2.html" data-scroll="section-2">Calculus</a></li>
<li><a href="section-3.html" data-scroll="section-3" class="active">Linear algebra</a></li>
<li><a href="section-4.html" data-scroll="section-4">Differential Equations</a></li>
</ul>
</li>
<li class="link">
<a href="ch-series.html" data-scroll="ch-series"><span class="codenumber">2</span> <span class="title">Series solutions</span></a><ul>
<li><a href="ch-series-1.html" data-scroll="ch-series-1">Power series</a></li>
<li><a href="ch-series-2.html" data-scroll="ch-series-2">Series solutions at ordinary points</a></li>
<li><a href="ch-series-3.html" data-scroll="ch-series-3">Legendre series</a></li>
<li><a href="ch-series-4.html" data-scroll="ch-series-4">Series solutions at regular singularities</a></li>
<li><a href="ch-series-5.html" data-scroll="ch-series-5">More on Frobenius methods (partial)</a></li>
<li><a href="ch-series-6.html" data-scroll="ch-series-6">Bessel equations</a></li>
<li><a href="ch-series-7.html" data-scroll="ch-series-7">Series as vectors (an introduction to \(L^2\))</a></li>
</ul>
</li>
<li class="link">
<a href="chapter-3.html" data-scroll="chapter-3"><span class="codenumber">3</span> <span class="title">All things Fourier</span></a><ul>
<li><a href="section-12.html" data-scroll="section-12">Periodicity</a></li>
<li><a href="section-13.html" data-scroll="section-13">Complex exponentials and trig functions</a></li>
<li><a href="section-14.html" data-scroll="section-14">Sums of periodic functions</a></li>
<li><a href="section-15.html" data-scroll="section-15">Questions of convergence</a></li>
<li><a href="section-16.html" data-scroll="section-16">First applications</a></li>
<li><a href="section-17.html" data-scroll="section-17">Fourier series and linear algebra</a></li>
<li><a href="section-18.html" data-scroll="section-18">The Fourier transform</a></li>
<li><a href="section-19.html" data-scroll="section-19">Convolution</a></li>
<li><a href="section-20.html" data-scroll="section-20">Applications of convolution</a></li>
</ul>
</li>
<li class="link">
<a href="ch-laplace.html" data-scroll="ch-laplace"><span class="codenumber">4</span> <span class="title">The Laplace Transform</span></a><ul>
<li><a href="ch-laplace-2.html" data-scroll="ch-laplace-2">The Laplace Transform</a></li>
<li><a href="ch-laplace-3.html" data-scroll="ch-laplace-3">Periodic functions</a></li>
<li><a href="ch-laplace-4.html" data-scroll="ch-laplace-4">The Laplace transform of derivatives</a></li>
<li><a href="ch-laplace-5.html" data-scroll="ch-laplace-5">The shifting theorems</a></li>
<li><a href="ch-laplace-6.html" data-scroll="ch-laplace-6">Heaviside functions</a></li>
<li><a href="ch-laplace-7.html" data-scroll="ch-laplace-7">Impulses - the Dirac delta “function”</a></li>
<li><a href="ch-laplace-8.html" data-scroll="ch-laplace-8">Convolution</a></li>
</ul>
</li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section xmlns:svg="http://www.w3.org/2000/svg" class="section" id="section-3"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">1.3</span> <span class="title">Linear algebra</span>
</h2>
<section class="subsection" id="sec-intro-1"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">1.3.1</span> <span class="title">Motivation</span>
</h3>
<p id="p-9">One of the most profound ideas of linear algebra is that <em class="emphasis">any finite dimensional vector space over \(\R\) or \(\C\) is secretly \(\R^n\) or \(\C^n\)</em>. This insight allows us to reduce the study of vector spaces and the maps between them to the study of matrices.</p>
<p id="p-10">The key idea is that every finite dimensional vector space can be represented in coordinates once we choose a basis. We denote the representation of a vector \(v \in V\) with respect to a basis \(\mathcal V\) by \(v_\mathcal{V}\text{.}\) Better yet, that basis can be chosen to be orthonormal by way of the Gram-Schmidt process and the dot product structure of Euclidean space. The coordinatization of \(V\) also gives unique representations of linear maps betwen those spaces.</p>
<article class="theorem theorem-like" id="theorem-1"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">1.3.1</span><span class="period">.</span>
</h6>
<p id="p-11">Let \(V, W\) be finite dimensional vector spaces with bases \(\mathcal V, \mathcal W\text{.}\) Then any linear map \(T: V \to W\) has a unique matrix representation with respect to \(\mathcal V, \mathcal W\) by</p>
<div class="displaymath">
\begin{equation*}
T(x) = A x
\end{equation*}
</div>
<p data-braille="continuation">with</p>
<div class="displaymath">
\begin{equation*}
A = \bbm T(v_1)_{\mathcal W} \amp \ldots \amp T(v_n)_{\mathcal W} \ebm
\end{equation*}
</div></article><p id="p-12">Typical examples introduced in a linear algebra course include the space of polynomials of degree less than or equal to \(n\text{.}\) At the same time, we usually also get to see a very suggestive example of a useful linear map and the representation of that map in matrix form.</p>
<p id="p-13">Let \(P_n\) denote the space of polynomials of degree \(\leq n\text{.}\) Consider the map \(D: P_3 \to P_2\) defined by</p>
<div class="displaymath">
\begin{equation*}
D(a_0 + a_1 t + a_2 t^2 + a_3 t^3) = a_1 + 2 a_2 t + 3 a_3 t^2.
\end{equation*}
</div>
<p data-braille="continuation">That is, \(D\) is the map that takes the derivative of a polynomial. It isn't hard to use the standard basis for \(P_n\) to get the matrix representation</p>
<div class="displaymath">
\begin{equation*}
D(p) = \bbm 0 \amp 1 \amp 0 \amp 0 \\ 0 \amp 0 \amp 2 \amp 0 \\ 0 \amp 0 \amp 0 \amp 3 \ebm \bbm a_0 \\ a_1 \\ a_2 \\ a_3 \ebm
\end{equation*}
</div>
<p data-braille="continuation">for the action of \(D\) on \(P_3\text{.}\)</p>
<p id="p-14">This example is a good place to begin asking questions about how far we can push finite dimensional linear algebra. The fact that differentiation of polynomials is wonderful - but what else can we apply it to? Nice functions have power series that converge absolutely, and we like to think of an absolutely convergent power series as sort of an “infinite polynomial”. Our intution might lead us to make a connection with calculus at this point. When we learn to work with power series, we learn that for a convergent power series,</p>
<div class="displaymath">
\begin{equation*}
\frac{d}{dx} \sum a_n (x-a)^n = \sum n a_n (x-a)^{n-1}.
\end{equation*}
</div>
<p data-braille="continuation">In analogy with our example about polynomials above, we're tempted to write, for a function \(f\) defined by a convergent power series, that</p>
<div class="displaymath">
\begin{equation*}
D(f) =  \underbrace{\bbm 0 \amp 1 \amp 0 \amp 0 \amp \ldots \\ 0 \amp 0 \amp 2 \amp 0 \amp \ldots \\ 0 \amp 0 \amp 0 \amp 3 \amp \ldots \\ \vdots \amp \amp \amp \amp \ddots \ebm}_{A} \bbm a_0 \\ a_1 \\ a_2 \\ a_3 \\ \vdots \ebm = a_1 + 2a_2 (x - a) + 3 a_3 (x-1)^2 + \ldots.
\end{equation*}
</div>
<p id="p-15">This idea is shot through with issues that need to be addressed.</p>
<ul class="disc">
<li id="li-19">The object \(A\) is some kind of \(\infty \times \infty\) matrix. How does that make sense?</li>
<li id="li-20">What are the vector spaces that \(D\) is mapping between?</li>
<li id="li-21">Does the idea of coordinatization still work?</li>
<li id="li-22">If it does, what exactly is “\(\R^\infty\)” supposed to be?</li>
<li id="li-23">Do infinite dimensional vector spaces and bases make sense at all?</li>
</ul></section><section class="subsection" id="subsection-2"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">1.3.2</span> <span class="title">Inner products</span>
</h3>
<p id="p-16">The <dfn class="terminology">dot product</dfn> of two vectors in \(\C^n\) is</p>
<div class="displaymath">
\begin{equation}
x \cdot y = \sum_{i=1}^n \cc y_i x_i.\label{def-dot}\tag{1.3.1}
\end{equation}
</div>
<p data-braille="continuation">Standard notation for the dot product is \(\ip{x}{y}\) and in \(\C^n\) is equivalent to \(y\ad x\text{,}\) where \(\ad\) designates the conjugate transpose of a matrix. The dot product has the following properties:</p>
<ol class="decimal">
<li id="li-24">\(\displaystyle \ip{x}{y} = \cc{\ip{y}{x}} \hspace{.2in} \text{conjugate symmetry}\)</li>
<li id="li-25">\(\displaystyle \ip{x + y}{z} = \ip{x}{y} + \ip{y}{z} \hspace{.2in} \text{linearity in the first term}\)</li>
<li id="li-26">\(\displaystyle \ip{x}{x} \geq 0 \hspace{.2in} \text{non-negativity}\)</li>
</ol>
<p id="p-17">Once we have the dot product, we can start building the geometry of \(\C^n\text{.}\) First, note that</p>
<div class="displaymath">
\begin{equation}
\norm{x}^2 = \ip{x}{x} = \sum_{i=1}^n \abs{x}^2.\label{eq-Euclidean-norm}\tag{1.3.2}
\end{equation}
</div>
<p data-braille="continuation">Motivated by the real case, we say that two vectors \(x, y\) are <dfn class="terminology">orthogonal</dfn> and write \(x \perp y\) if \(\ip{x}{y} = 0\text{.}\)</p>
<p id="p-18">Another important inequality is indicated by the relationship between angles and the dot product in \(\R^n\text{,}\) where we have</p>
<div class="displaymath">
\begin{equation*}
\ip{x}{y} = \norm{x}\norm{y}\cos \theta,
\end{equation*}
</div>
<p data-braille="continuation">where \(\theta\) is the angle between the vectors. While the idea of “angle” doesn't make sense in \(\C^n\) (at least in the same way), we still have the <dfn class="terminology">Cauchy-Schwarz</dfn> inequality</p>
<div class="displaymath">
\begin{equation}
\abs{\ip{x}{y}} \leq \norm{x}\norm{y}.\tag{1.3.3}
\end{equation}
</div>
<p id="p-19">Orthogonality also underlies the vector version of the <dfn class="terminology">Pythagorean theorem</dfn>,</p>
<div class="displaymath">
\begin{equation}
\norm{x}^2 + \norm{y}^2 = \norm{x+ y}^2 \iff x\perp y.\tag{1.3.4}
\end{equation}
</div>
<p id="p-20">Finally, it would be remiss to leave out the single most important inequality in mathematics, our old friend the <dfn class="terminology">triangle inequality</dfn>, which in vector terms can be expressed</p>
<div class="displaymath">
\begin{equation}
\norm{x + y} \leq \norm{x} + \norm{y}\tag{1.3.5}
\end{equation}
</div>
<p id="p-21">Because finite dimensional vector spaces have representations in coordinates as \(\R^n\) or \(\C^n\text{,}\) all finite dimensional vector spaces carry the geometric structure delineated above.</p></section><section class="subsection" id="intro-2"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">1.3.3</span> <span class="title">Basis and coordinates</span>
</h3>
<p id="p-22">Let \(V\) be a vector space over a field \(\F\text{.}\) Recall that a (finite) set of vectors \(S \subset V\) is <dfn class="terminology">linearly independent</dfn> if only the trivial solution exists for the equation</p>
<div class="displaymath">
\begin{equation}
0 = \sum_\mathcal{I} c_i v_i.\tag{1.3.6}
\end{equation}
</div>
<p data-braille="continuation">A set \(S\) of vectors in \(V\) is said to <dfn class="terminology">span</dfn> \(V\) if every vector in \(V\) can be realized as a linear combination of vectors in \(S\text{.}\) That is, given \(v \in V\text{,}\) there exist coefficients \(c_i\) so that</p>
<div class="displaymath">
\begin{equation*}
v = \sum_{\mathcal I} c_i v_i.
\end{equation*}
</div>
<p id="p-23">A basis \(\mathcal V\) for \(V\) is a subset of \(V\) so that \(\mathcal V\) is linearly independent and \(\mathcal V\) spans \(V\text{.}\) It is a major result that every vector space has a basis. The full result requires the invocation of <a class="external" href="https://en.wikipedia.org/wiki/Zorn%27s_lemma" target="_blank">Zorn's Lemma</a> or other equivalents of the <a class="external" href="https://en.wikipedia.org/wiki/Axiom_of_choice" target="_blank">axiom of choice</a> and will not be proven here. (A nice argument can be found <a class="external" href="http://www.math.lsa.umich.edu/~kesmith/infinite.pdf" target="_blank">here</a>.) Our interest is in modeling vector spaces the carry the logic and structure of Euclidean space. The <dfn class="terminology">dimension</dfn> of \(V\) is the order of a basis \(\mathcal V\text{.}\) If the basis has a finite number of elements, say \(n\text{,}\) then \(V\) is called finite dimensional. In particular, (and clearly providing motivation for the definition), \(\dim \R^n = n\text{.}\)</p>
<p id="p-24">Suppose that \(V\) is a finite dimensional vector space with a basis \(\mathcal V\text{.}\) Let \(v\) be a vector in \(V\text{.}\) Then the <dfn class="terminology">coordinates of \(v\) with respect to \(\mathcal V\)</dfn> are the constants \(c_i\) so that \(v = \sum_{\mathcal I} c_i v_i\text{.}\) These coordinates are <em class="emphasis">unique</em> once we have fixed a basis \(\mathcal V\text{.}\) That is, we have a bijective relationship between the vectors \(v \in V\) and the coordinate representations \(\bbm c_1 \\ \vdots \\ c_n \ebm \in \F^n\text{.}\) In \(\F^n\text{,}\) the coordinate representation of a vector is straightforward to compute using the dot product.</p>
<article class="theorem theorem-like" id="thm-finite-coords"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">1.3.2</span><span class="period">.</span>
</h6>
<p id="p-25">Let \(e_1, \ldots e_m\) be an orthonormal basis for \(\F^m\) and \(v \in \F^n\text{.}\) Then the \(n\)th coordinate of \(v\) with respect to the basis is \(\ip{v}{e_n}\text{,}\) and the expansion of \(v\) with respect to the basis is</p>
<div class="displaymath">
\begin{equation*}
v = \sum_{1}^m \ip{v}{e_i} e_i.
\end{equation*}
</div></article><p id="p-26">Furthermore, we can use the coordinate representation to write representing matrices for linear functions \(T:V \to W\text{.}\) Suppose that \(V, W\) are vector spaces of dimension \(m,n\) respectively over \(\F\text{.}\) Then <div class="sidebyside"><div class="sbsrow" style="margin-left:35%;margin-right:35%;"><div class="sbspanel" style="width:100%;justify-content:flex-start;"><img src="images/image-1.svg" role="img" style="width: 100%; height: auto;" alt=""></div></div></div> where \(A\) is the matrix that represents \(T\) and \(i\) is the natural bijection- the coordinatization - between \(V, W\) and \(\F^m, \F^n\) respectively. We should note that matrix multiplication is defined so that <div class="sidebyside"><div class="sbsrow" style="margin-left:30%;margin-right:30%;"><div class="sbspanel" style="width:100%;justify-content:flex-start;"><img src="images/image-2.svg" role="img" style="width: 100%; height: auto;" alt=""></div></div></div> reduces to the diagram <div class="sidebyside"><div class="sbsrow" style="margin-left:35%;margin-right:35%;"><div class="sbspanel" style="width:100%;justify-content:flex-start;"><img src="images/image-3.svg" role="img" style="width: 100%; height: auto;" alt=""></div></div></div> That is, the representing matrix of a composition is the product of the representing matrices of the functions.</p>
<p id="p-27">Any basis of a vector space can be replaced with an equivalent basis of orthonormal vectors - the algorithm for creating an orthonormal basis from a basis is called the <dfn class="terminology"><a class="external" href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process" target="_blank">Gram-Schmidt process</a></dfn>.</p></section><section class="subsection" id="subsection-4"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">1.3.4</span> <span class="title">Operators</span>
</h3>
<p id="p-28">When a linear function maps \(V\) into itself, special things happen. First, the matrix that represents \(T: \F^n \to \F^n\) is square. There are a large number of equivalences between the structure of square matrices, linear maps, and sets of vectors. Many of these are captured in the <dfn class="terminology">invertible matrix theorem</dfn>, one of the central objects of study in elementary linear algebra.</p>
<article class="theorem theorem-like" id="thminvmat1"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">1.3.3</span><span class="period">.</span><span class="space"> </span><span class="title">Invertible matrix theorem.</span>
</h6>
<p id="p-29">Let \(A\) be an \(n \times n\) matrix. If any of the following conditions hold, all of them do. If any of them are false, they are all.</p>
<ol class="decimal">
<li id="li-27">\(A\) is invertible.</li>
<li id="li-28">\(A\) row reduces to the identity matrix \(I\text{.}\)</li>
<li id="li-29">\(A\) has \(n\) pivot positions.</li>
<li id="li-30">\(rank A = n\text{.}\)</li>
<li id="li-31">The equation \(A \vec x = \vec 0\) has only the trivial solution.</li>
<li id="li-32">The columns of \(A\) are linearly independent.</li>
<li id="li-33">The function \(T(\vec x) = A \vec x\) is one-to-one.</li>
<li id="li-34">The equation \(A \vec x = \vec v\) is consistent for all \(b \in \F^n\text{.}\)</li>
<li id="li-35">The columns of \(A\) span \(\F^n\text{.}\)</li>
<li id="li-36">The function \(T(\vec x) = A \vec x\) is onto.</li>
<li id="li-37">There is a matrix \(C\) so that \(C A = I\text{.}\)</li>
<li id="li-38">There is a matrix \(D\) so that \(A D = I\text{.}\)</li>
<li id="li-39">\(A^T\) is invertible.</li>
<li id="li-40">\(\displaystyle \det A \neq 0.\)</li>
</ol></article><p id="p-30">Operators contain more information than the invertibility of the functions that they represent. For the following discussion, let us fix a basis of a vector space \(V\) and let \(A\) be the matrix that represents a function \(T: V \to V\text{.}\) A scalar \(\la\) and a vector \(v\) are said to be an <dfn class="terminology">eigenpair</dfn> for \(A\) if</p>
<div class="displaymath">
\begin{equation*}
A v = \la v.
\end{equation*}
</div>
<p data-braille="continuation">It is straightforward to see that the set of all vectors \(v\) for which the eigenvector equation holds is a subspace of \(V\text{,}\) called the <dfn class="terminology">eigenspace</dfn> associated with \(\la\text{.}\) The eigenspaces of the matrix \(A\) are its <dfn class="terminology">invariant subspaces</dfn>, which is to say that a vector in an eigenspace is mapped by \(A\) to the same eigenspace. It turns out that knowing the invariant subspaces of \(A\) are often enough to completely characterize \(A\text{.}\) If \(A\) is \(n\times n\) and \(A\) has \(n\) linearly independent eigenvectors (that is, one can find a basis of \(\F^n\) consisting of eigenvectors of \(A\)), then</p>
<div class="displaymath">
\begin{equation*}
A = S D S\inv,
\end{equation*}
</div>
<p data-braille="continuation">where \(S\) is a matrix of eigenvectors and \(D\) is a diagonal matrix of the associated eigenvalues (including repetition of course). (One should think of \(S\) as a change of basis matrix under which the operator \(A\) becomes diagonal.)</p>
<p id="p-31">Many operators are not diagonalizable, even very simple ones. For example, \(A = \bbm 1 \amp 1 \\ 0 \amp 1 \ebm\) only has a one-dimensional eigenspace. Diagonalizability is so useful that we give characterizations of those operators a special name, the <dfn class="terminology">Spectral Theorem</dfn>. An operator on a real vector space is called <dfn class="terminology">symmetric</dfn> if \(A^T = A\text{.}\) An operator on a complex vector space is called <dfn class="terminology">Hermitian</dfn> (or conjugate symmetric) if \(A\ad = \cc{A^T} = A\text{.}\) One of the major theorems of elementary linear algebra is that such operators are diagonalizable and that there exists an orthonormal basis of eigenvectors for \(V\text{.}\)</p>
<article class="theorem theorem-like" id="theorem-4"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">1.3.4</span><span class="period">.</span>
</h6>
<p id="p-32">Let \(A\) be an \(n \times n\) real (complex) matrix. Then \(A\) is diagonalizable with respect to an orthonormal basis of eigenvectors if and only if \(A\) is symmetric (hermitian).</p></article><p id="p-33">For complex operators, one can say more. \(A\) is called <dfn class="terminology">normal</dfn> if \(A A\ad = A\ad A\text{.}\) One reason that complex vector spaces are so much nicer than real vector spaces is that normal operators turn out to have orthonormal diagonalizations.</p>
<article class="theorem theorem-like" id="theorem-5"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">1.3.5</span><span class="period">.</span><span class="space"> </span><span class="title">Complex (finite) spectral theorem.</span>
</h6>
<p id="p-34">Let \(A\) be an operator on a finite dimensional Hilbert space \(V\text{.}\) Then \(A\) is normal if and only if \(A\) can be diagonalized with respect to an orthonormal basis of eigenvectors for \(V\text{.}\)</p></article></section></section></div></main>
</div>
</body>
</html>
